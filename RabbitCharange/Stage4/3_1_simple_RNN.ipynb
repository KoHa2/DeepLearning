{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "name": "3_1_simple_RNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KoHa2/DeepLearning/blob/main/RabbitCharange/Stage4/3_1_simple_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cNl2QA_Rnv5"
      },
      "source": [
        "# 準備"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkwjN1jNVAYy"
      },
      "source": [
        "## Googleドライブのマウント"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvFXpiH3EVC1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25519aaa-ac4e-489d-dd44-df6c4cea4eb1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ub7RYdeY6pK"
      },
      "source": [
        "## sys.pathの設定"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oql7L19rEsWi"
      },
      "source": [
        "以下では，Googleドライブのマイドライブ直下にDNN_codeフォルダを置くことを仮定しています．必要に応じて，パスを変更してください．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ic2JzkvFX59"
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/E/04.stage4/DNN_code_colab_lesson_3_4')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feXB1SiLP4OL"
      },
      "source": [
        "# simple RNN\n",
        "### バイナリ加算"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "tzSWNYwxP4OM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ad042889-2ee0-4f8f-e65c-1a1e4b98c8e9"
      },
      "source": [
        "import numpy as np\n",
        "from common import functions\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# def d_tanh(x):\n",
        "\n",
        "\n",
        "\n",
        "# データを用意\n",
        "# 2進数の桁数\n",
        "binary_dim = 8\n",
        "# 最大値 + 1\n",
        "largest_number = pow(2, binary_dim)\n",
        "# largest_numberまで2進数を用意\n",
        "binary = np.unpackbits(np.array([range(largest_number)],dtype=np.uint8).T,axis=1)\n",
        "\n",
        "input_layer_size = 2\n",
        "hidden_layer_size = 16\n",
        "output_layer_size = 1\n",
        "\n",
        "weight_init_std = 1\n",
        "learning_rate = 0.1\n",
        "\n",
        "iters_num = 10000\n",
        "plot_interval = 100\n",
        "\n",
        "# ウェイト初期化 (バイアスは簡単のため省略)\n",
        "W_in = weight_init_std * np.random.randn(input_layer_size, hidden_layer_size)\n",
        "W_out = weight_init_std * np.random.randn(hidden_layer_size, output_layer_size)\n",
        "W = weight_init_std * np.random.randn(hidden_layer_size, hidden_layer_size)\n",
        "\n",
        "# Xavier\n",
        "\n",
        "\n",
        "# He\n",
        "\n",
        "\n",
        "\n",
        "# 勾配\n",
        "W_in_grad = np.zeros_like(W_in)\n",
        "W_out_grad = np.zeros_like(W_out)\n",
        "W_grad = np.zeros_like(W)\n",
        "\n",
        "u = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "z = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "y = np.zeros((output_layer_size, binary_dim))\n",
        "\n",
        "delta_out = np.zeros((output_layer_size, binary_dim))\n",
        "delta = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "\n",
        "all_losses = []\n",
        "\n",
        "for i in range(iters_num):\n",
        "    \n",
        "    # A, B初期化 (a + b = d)\n",
        "    a_int = np.random.randint(largest_number/2)\n",
        "    a_bin = binary[a_int] # binary encoding\n",
        "    b_int = np.random.randint(largest_number/2)\n",
        "    b_bin = binary[b_int] # binary encoding\n",
        "    \n",
        "    # 正解データ\n",
        "    d_int = a_int + b_int\n",
        "    d_bin = binary[d_int]\n",
        "    \n",
        "    # 出力バイナリ\n",
        "    out_bin = np.zeros_like(d_bin)\n",
        "    \n",
        "    # 時系列全体の誤差\n",
        "    all_loss = 0    \n",
        "    \n",
        "    # 時系列ループ\n",
        "    for t in range(binary_dim):\n",
        "        # 入力値\n",
        "        X = np.array([a_bin[ - t - 1], b_bin[ - t - 1]]).reshape(1, -1)\n",
        "        # 時刻tにおける正解データ\n",
        "        dd = np.array([d_bin[binary_dim - t - 1]])\n",
        "        \n",
        "        u[:,t+1] = np.dot(X, W_in) + np.dot(z[:,t].reshape(1, -1), W)\n",
        "        z[:,t+1] = functions.sigmoid(u[:,t+1])\n",
        "\n",
        "        y[:,t] = functions.sigmoid(np.dot(z[:,t+1].reshape(1, -1), W_out))\n",
        "\n",
        "\n",
        "        #誤差\n",
        "        loss = functions.mean_squared_error(dd, y[:,t])\n",
        "        \n",
        "        delta_out[:,t] = functions.d_mean_squared_error(dd, y[:,t]) * functions.d_sigmoid(y[:,t])        \n",
        "        \n",
        "        all_loss += loss\n",
        "\n",
        "        out_bin[binary_dim - t - 1] = np.round(y[:,t])\n",
        "    \n",
        "    \n",
        "    for t in range(binary_dim)[::-1]:\n",
        "        X = np.array([a_bin[-t-1],b_bin[-t-1]]).reshape(1, -1)        \n",
        "\n",
        "        delta[:,t] = (np.dot(delta[:,t+1].T, W.T) + np.dot(delta_out[:,t].T, W_out.T)) * functions.d_sigmoid(u[:,t+1])\n",
        "\n",
        "        # 勾配更新\n",
        "        W_out_grad += np.dot(z[:,t+1].reshape(-1,1), delta_out[:,t].reshape(-1,1))\n",
        "        W_grad += np.dot(z[:,t].reshape(-1,1), delta[:,t].reshape(1,-1))\n",
        "        W_in_grad += np.dot(X.T, delta[:,t].reshape(1,-1))\n",
        "    \n",
        "    # 勾配適用\n",
        "    W_in -= learning_rate * W_in_grad\n",
        "    W_out -= learning_rate * W_out_grad\n",
        "    W -= learning_rate * W_grad\n",
        "    \n",
        "    W_in_grad *= 0\n",
        "    W_out_grad *= 0\n",
        "    W_grad *= 0\n",
        "    \n",
        "\n",
        "    if(i % plot_interval == 0):\n",
        "        all_losses.append(all_loss)        \n",
        "        print(\"iters:\" + str(i))\n",
        "        print(\"Loss:\" + str(all_loss))\n",
        "        print(\"Pred:\" + str(out_bin))\n",
        "        print(\"True:\" + str(d_bin))\n",
        "        out_int = 0\n",
        "        for index,x in enumerate(reversed(out_bin)):\n",
        "            out_int += x * pow(2, index)\n",
        "        print(str(a_int) + \" + \" + str(b_int) + \" = \" + str(out_int))\n",
        "        print(\"------------\")\n",
        "\n",
        "lists = range(0, iters_num, plot_interval)\n",
        "plt.plot(lists, all_losses, label=\"loss\")\n",
        "plt.show()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iters:0\n",
            "Loss:2.078596544144684\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 1 1 1 0 1]\n",
            "75 + 50 = 0\n",
            "------------\n",
            "iters:100\n",
            "Loss:0.9467963711356286\n",
            "Pred:[0 1 1 0 1 0 0 0]\n",
            "True:[0 1 0 1 0 0 1 0]\n",
            "10 + 72 = 104\n",
            "------------\n",
            "iters:200\n",
            "Loss:1.1628458053079105\n",
            "Pred:[0 1 1 1 1 1 1 1]\n",
            "True:[1 0 0 1 0 1 1 0]\n",
            "87 + 63 = 127\n",
            "------------\n",
            "iters:300\n",
            "Loss:0.9095275244260828\n",
            "Pred:[0 0 0 0 0 0 1 1]\n",
            "True:[0 1 1 0 0 1 1 1]\n",
            "82 + 21 = 3\n",
            "------------\n",
            "iters:400\n",
            "Loss:0.9338901672023385\n",
            "Pred:[1 0 1 0 0 1 1 1]\n",
            "True:[1 0 1 0 0 0 1 1]\n",
            "72 + 91 = 167\n",
            "------------\n",
            "iters:500\n",
            "Loss:1.1263492185523951\n",
            "Pred:[0 1 1 1 1 1 1 1]\n",
            "True:[1 0 1 0 0 1 0 1]\n",
            "110 + 55 = 127\n",
            "------------\n",
            "iters:600\n",
            "Loss:1.0040128584642616\n",
            "Pred:[0 0 0 0 0 0 0 1]\n",
            "True:[0 1 1 1 0 0 1 1]\n",
            "3 + 112 = 1\n",
            "------------\n",
            "iters:700\n",
            "Loss:0.8808532443102961\n",
            "Pred:[1 0 1 1 0 0 0 0]\n",
            "True:[1 0 0 0 0 1 0 0]\n",
            "56 + 76 = 176\n",
            "------------\n",
            "iters:800\n",
            "Loss:0.9637850559380128\n",
            "Pred:[0 0 0 1 0 0 0 0]\n",
            "True:[0 1 1 0 0 0 0 1]\n",
            "52 + 45 = 16\n",
            "------------\n",
            "iters:900\n",
            "Loss:1.1049272423307905\n",
            "Pred:[1 1 0 0 1 1 0 1]\n",
            "True:[0 1 1 1 0 0 0 1]\n",
            "11 + 102 = 205\n",
            "------------\n",
            "iters:1000\n",
            "Loss:1.1649688142946435\n",
            "Pred:[0 1 1 1 1 0 1 1]\n",
            "True:[1 0 0 0 0 0 1 1]\n",
            "95 + 36 = 123\n",
            "------------\n",
            "iters:1100\n",
            "Loss:1.227832521278708\n",
            "Pred:[1 1 1 1 1 0 0 1]\n",
            "True:[1 0 0 0 0 1 1 0]\n",
            "55 + 79 = 249\n",
            "------------\n",
            "iters:1200\n",
            "Loss:1.0078403868635044\n",
            "Pred:[1 0 0 1 1 1 0 0]\n",
            "True:[1 0 1 0 0 0 0 0]\n",
            "62 + 98 = 156\n",
            "------------\n",
            "iters:1300\n",
            "Loss:0.5059794322785839\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 0 0 0 1 0]\n",
            "2 + 64 = 0\n",
            "------------\n",
            "iters:1400\n",
            "Loss:1.1383154785235907\n",
            "Pred:[1 1 0 1 0 1 0 0]\n",
            "True:[1 0 1 0 0 0 1 1]\n",
            "121 + 42 = 212\n",
            "------------\n",
            "iters:1500\n",
            "Loss:1.114102785457242\n",
            "Pred:[0 1 1 1 0 0 0 1]\n",
            "True:[1 0 0 0 1 0 1 1]\n",
            "108 + 31 = 113\n",
            "------------\n",
            "iters:1600\n",
            "Loss:1.1254255700084665\n",
            "Pred:[1 1 1 1 0 1 0 1]\n",
            "True:[1 0 0 0 1 0 0 1]\n",
            "110 + 27 = 245\n",
            "------------\n",
            "iters:1700\n",
            "Loss:0.6233842623331065\n",
            "Pred:[1 0 1 1 0 1 1 1]\n",
            "True:[1 0 1 1 0 1 1 1]\n",
            "68 + 115 = 183\n",
            "------------\n",
            "iters:1800\n",
            "Loss:0.7304217424923063\n",
            "Pred:[1 1 0 0 0 0 1 1]\n",
            "True:[1 0 0 0 0 0 1 1]\n",
            "80 + 51 = 195\n",
            "------------\n",
            "iters:1900\n",
            "Loss:0.3971183035875176\n",
            "Pred:[0 1 0 1 0 1 0 0]\n",
            "True:[0 1 0 1 0 1 0 0]\n",
            "50 + 34 = 84\n",
            "------------\n",
            "iters:2000\n",
            "Loss:0.8780983203157687\n",
            "Pred:[0 1 1 0 0 0 1 0]\n",
            "True:[1 0 0 0 0 0 1 0]\n",
            "113 + 17 = 98\n",
            "------------\n",
            "iters:2100\n",
            "Loss:0.7223888909348192\n",
            "Pred:[1 1 1 0 1 1 1 0]\n",
            "True:[1 0 1 0 1 1 0 0]\n",
            "115 + 57 = 238\n",
            "------------\n",
            "iters:2200\n",
            "Loss:0.60823926406603\n",
            "Pred:[0 1 1 1 1 0 0 1]\n",
            "True:[0 1 1 1 1 0 0 1]\n",
            "71 + 50 = 121\n",
            "------------\n",
            "iters:2300\n",
            "Loss:0.8365634679661046\n",
            "Pred:[1 0 0 0 0 0 0 1]\n",
            "True:[1 0 1 0 1 1 0 1]\n",
            "63 + 110 = 129\n",
            "------------\n",
            "iters:2400\n",
            "Loss:0.9344246442710693\n",
            "Pred:[0 1 0 0 1 1 0 0]\n",
            "True:[1 0 0 0 0 0 0 0]\n",
            "61 + 67 = 76\n",
            "------------\n",
            "iters:2500\n",
            "Loss:0.7822439780326391\n",
            "Pred:[1 0 0 1 1 1 0 0]\n",
            "True:[1 1 0 1 1 1 1 0]\n",
            "127 + 95 = 156\n",
            "------------\n",
            "iters:2600\n",
            "Loss:0.46442274030930797\n",
            "Pred:[1 1 0 1 0 1 0 0]\n",
            "True:[1 0 0 1 0 1 0 0]\n",
            "92 + 56 = 212\n",
            "------------\n",
            "iters:2700\n",
            "Loss:0.45988409809382685\n",
            "Pred:[1 0 1 0 0 0 0 1]\n",
            "True:[1 0 1 1 0 0 0 1]\n",
            "83 + 94 = 161\n",
            "------------\n",
            "iters:2800\n",
            "Loss:0.11790990095418195\n",
            "Pred:[0 0 1 1 1 0 0 0]\n",
            "True:[0 0 1 1 1 0 0 0]\n",
            "34 + 22 = 56\n",
            "------------\n",
            "iters:2900\n",
            "Loss:0.7489300270911929\n",
            "Pred:[1 0 0 1 0 0 0 0]\n",
            "True:[1 0 0 1 0 1 0 0]\n",
            "63 + 85 = 144\n",
            "------------\n",
            "iters:3000\n",
            "Loss:0.027884130829458544\n",
            "Pred:[0 1 1 0 0 1 0 0]\n",
            "True:[0 1 1 0 0 1 0 0]\n",
            "64 + 36 = 100\n",
            "------------\n",
            "iters:3100\n",
            "Loss:0.46591698136649495\n",
            "Pred:[1 0 1 0 0 1 1 0]\n",
            "True:[1 1 0 0 0 1 1 0]\n",
            "109 + 89 = 166\n",
            "------------\n",
            "iters:3200\n",
            "Loss:0.2021173128327192\n",
            "Pred:[1 0 0 1 1 1 1 1]\n",
            "True:[1 1 0 1 1 1 1 1]\n",
            "126 + 97 = 159\n",
            "------------\n",
            "iters:3300\n",
            "Loss:0.13625626562360935\n",
            "Pred:[0 1 0 1 1 0 1 1]\n",
            "True:[0 1 0 1 1 0 1 1]\n",
            "87 + 4 = 91\n",
            "------------\n",
            "iters:3400\n",
            "Loss:0.4059974489321174\n",
            "Pred:[0 1 1 0 0 0 0 1]\n",
            "True:[0 1 1 0 1 0 0 1]\n",
            "62 + 43 = 97\n",
            "------------\n",
            "iters:3500\n",
            "Loss:0.20117292929691702\n",
            "Pred:[1 0 1 1 0 1 0 0]\n",
            "True:[1 0 1 1 0 1 1 0]\n",
            "75 + 107 = 180\n",
            "------------\n",
            "iters:3600\n",
            "Loss:0.2602193777461053\n",
            "Pred:[1 1 1 0 1 0 1 0]\n",
            "True:[1 1 1 0 1 0 1 0]\n",
            "124 + 110 = 234\n",
            "------------\n",
            "iters:3700\n",
            "Loss:0.05575476779938524\n",
            "Pred:[1 0 0 1 1 1 0 1]\n",
            "True:[1 0 0 1 1 1 0 1]\n",
            "41 + 116 = 157\n",
            "------------\n",
            "iters:3800\n",
            "Loss:0.14651289352069308\n",
            "Pred:[1 0 1 0 0 1 0 0]\n",
            "True:[1 0 1 0 0 1 0 0]\n",
            "86 + 78 = 164\n",
            "------------\n",
            "iters:3900\n",
            "Loss:0.06618195717764494\n",
            "Pred:[0 1 1 0 1 1 1 1]\n",
            "True:[0 1 1 0 1 1 1 1]\n",
            "50 + 61 = 111\n",
            "------------\n",
            "iters:4000\n",
            "Loss:0.153412667018019\n",
            "Pred:[1 0 0 1 0 0 0 1]\n",
            "True:[1 0 0 1 0 0 0 1]\n",
            "55 + 90 = 145\n",
            "------------\n",
            "iters:4100\n",
            "Loss:0.033337693716226816\n",
            "Pred:[0 1 1 0 0 1 0 1]\n",
            "True:[0 1 1 0 0 1 0 1]\n",
            "98 + 3 = 101\n",
            "------------\n",
            "iters:4200\n",
            "Loss:0.05500160222891585\n",
            "Pred:[0 1 1 1 1 0 1 1]\n",
            "True:[0 1 1 1 1 0 1 1]\n",
            "25 + 98 = 123\n",
            "------------\n",
            "iters:4300\n",
            "Loss:0.11046430697219954\n",
            "Pred:[1 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 0 0 0 0 0]\n",
            "19 + 109 = 128\n",
            "------------\n",
            "iters:4400\n",
            "Loss:0.032329878181639425\n",
            "Pred:[0 0 1 1 0 1 0 1]\n",
            "True:[0 0 1 1 0 1 0 1]\n",
            "5 + 48 = 53\n",
            "------------\n",
            "iters:4500\n",
            "Loss:0.03179161975625786\n",
            "Pred:[0 1 1 0 1 1 0 1]\n",
            "True:[0 1 1 0 1 1 0 1]\n",
            "77 + 32 = 109\n",
            "------------\n",
            "iters:4600\n",
            "Loss:0.010720910339033062\n",
            "Pred:[0 1 1 1 1 0 1 0]\n",
            "True:[0 1 1 1 1 0 1 0]\n",
            "68 + 54 = 122\n",
            "------------\n",
            "iters:4700\n",
            "Loss:0.11584571235045646\n",
            "Pred:[1 1 1 0 0 0 0 1]\n",
            "True:[1 1 1 0 0 0 0 1]\n",
            "123 + 102 = 225\n",
            "------------\n",
            "iters:4800\n",
            "Loss:0.03928315575342813\n",
            "Pred:[0 0 1 1 1 1 1 1]\n",
            "True:[0 0 1 1 1 1 1 1]\n",
            "28 + 35 = 63\n",
            "------------\n",
            "iters:4900\n",
            "Loss:0.017467184085497686\n",
            "Pred:[0 0 0 0 1 0 1 0]\n",
            "True:[0 0 0 0 1 0 1 0]\n",
            "5 + 5 = 10\n",
            "------------\n",
            "iters:5000\n",
            "Loss:0.0638118092309194\n",
            "Pred:[1 0 1 0 1 0 0 0]\n",
            "True:[1 0 1 0 1 0 0 0]\n",
            "91 + 77 = 168\n",
            "------------\n",
            "iters:5100\n",
            "Loss:0.04118947240798267\n",
            "Pred:[1 0 1 0 1 0 1 0]\n",
            "True:[1 0 1 0 1 0 1 0]\n",
            "83 + 87 = 170\n",
            "------------\n",
            "iters:5200\n",
            "Loss:0.05104383467278855\n",
            "Pred:[0 1 1 1 1 1 1 0]\n",
            "True:[0 1 1 1 1 1 1 0]\n",
            "99 + 27 = 126\n",
            "------------\n",
            "iters:5300\n",
            "Loss:0.013790167140126009\n",
            "Pred:[1 0 0 1 1 1 0 0]\n",
            "True:[1 0 0 1 1 1 0 0]\n",
            "114 + 42 = 156\n",
            "------------\n",
            "iters:5400\n",
            "Loss:0.02776375542571701\n",
            "Pred:[1 0 0 0 0 1 0 1]\n",
            "True:[1 0 0 0 0 1 0 1]\n",
            "59 + 74 = 133\n",
            "------------\n",
            "iters:5500\n",
            "Loss:0.038618444037897236\n",
            "Pred:[1 0 1 1 1 0 0 0]\n",
            "True:[1 0 1 1 1 0 0 0]\n",
            "109 + 75 = 184\n",
            "------------\n",
            "iters:5600\n",
            "Loss:0.006974298387952642\n",
            "Pred:[1 0 0 0 1 1 0 0]\n",
            "True:[1 0 0 0 1 1 0 0]\n",
            "44 + 96 = 140\n",
            "------------\n",
            "iters:5700\n",
            "Loss:0.013873684511417817\n",
            "Pred:[0 1 0 1 1 0 0 1]\n",
            "True:[0 1 0 1 1 0 0 1]\n",
            "6 + 83 = 89\n",
            "------------\n",
            "iters:5800\n",
            "Loss:0.004596274833068794\n",
            "Pred:[0 1 0 0 1 1 1 0]\n",
            "True:[0 1 0 0 1 1 1 0]\n",
            "0 + 78 = 78\n",
            "------------\n",
            "iters:5900\n",
            "Loss:0.03285371331493147\n",
            "Pred:[1 1 0 0 0 1 1 0]\n",
            "True:[1 1 0 0 0 1 1 0]\n",
            "125 + 73 = 198\n",
            "------------\n",
            "iters:6000\n",
            "Loss:0.015952526712818497\n",
            "Pred:[0 1 0 0 1 1 0 1]\n",
            "True:[0 1 0 0 1 1 0 1]\n",
            "15 + 62 = 77\n",
            "------------\n",
            "iters:6100\n",
            "Loss:0.006585376327396322\n",
            "Pred:[0 1 0 0 0 1 1 0]\n",
            "True:[0 1 0 0 0 1 1 0]\n",
            "28 + 42 = 70\n",
            "------------\n",
            "iters:6200\n",
            "Loss:0.0051704977146365206\n",
            "Pred:[0 0 1 1 1 0 0 0]\n",
            "True:[0 0 1 1 1 0 0 0]\n",
            "48 + 8 = 56\n",
            "------------\n",
            "iters:6300\n",
            "Loss:0.007698188270978837\n",
            "Pred:[0 1 0 1 1 0 0 1]\n",
            "True:[0 1 0 1 1 0 0 1]\n",
            "49 + 40 = 89\n",
            "------------\n",
            "iters:6400\n",
            "Loss:0.009515067318066308\n",
            "Pred:[0 1 1 0 1 1 1 1]\n",
            "True:[0 1 1 0 1 1 1 1]\n",
            "24 + 87 = 111\n",
            "------------\n",
            "iters:6500\n",
            "Loss:0.005844674716733619\n",
            "Pred:[0 1 1 1 1 0 1 1]\n",
            "True:[0 1 1 1 1 0 1 1]\n",
            "101 + 22 = 123\n",
            "------------\n",
            "iters:6600\n",
            "Loss:0.0045130262234854696\n",
            "Pred:[0 1 1 0 0 0 1 0]\n",
            "True:[0 1 1 0 0 0 1 0]\n",
            "25 + 73 = 98\n",
            "------------\n",
            "iters:6700\n",
            "Loss:0.008400491284905052\n",
            "Pred:[0 1 1 1 1 0 1 1]\n",
            "True:[0 1 1 1 1 0 1 1]\n",
            "95 + 28 = 123\n",
            "------------\n",
            "iters:6800\n",
            "Loss:0.006963201985381021\n",
            "Pred:[1 0 1 0 1 1 1 1]\n",
            "True:[1 0 1 0 1 1 1 1]\n",
            "49 + 126 = 175\n",
            "------------\n",
            "iters:6900\n",
            "Loss:0.0032767547703734757\n",
            "Pred:[0 0 1 1 0 0 0 0]\n",
            "True:[0 0 1 1 0 0 0 0]\n",
            "4 + 44 = 48\n",
            "------------\n",
            "iters:7000\n",
            "Loss:0.011223288525848488\n",
            "Pred:[1 0 0 1 0 0 0 0]\n",
            "True:[1 0 0 1 0 0 0 0]\n",
            "124 + 20 = 144\n",
            "------------\n",
            "iters:7100\n",
            "Loss:0.005699859800078798\n",
            "Pred:[1 1 0 1 0 0 1 1]\n",
            "True:[1 1 0 1 0 0 1 1]\n",
            "110 + 101 = 211\n",
            "------------\n",
            "iters:7200\n",
            "Loss:0.005140375391451261\n",
            "Pred:[1 0 0 0 1 1 1 1]\n",
            "True:[1 0 0 0 1 1 1 1]\n",
            "73 + 70 = 143\n",
            "------------\n",
            "iters:7300\n",
            "Loss:0.0036103614587153906\n",
            "Pred:[1 0 0 0 1 0 0 1]\n",
            "True:[1 0 0 0 1 0 0 1]\n",
            "66 + 71 = 137\n",
            "------------\n",
            "iters:7400\n",
            "Loss:0.0027551220858786397\n",
            "Pred:[1 0 1 0 1 1 0 0]\n",
            "True:[1 0 1 0 1 1 0 0]\n",
            "64 + 108 = 172\n",
            "------------\n",
            "iters:7500\n",
            "Loss:0.004199744648484142\n",
            "Pred:[0 1 1 1 0 1 0 1]\n",
            "True:[0 1 1 1 0 1 0 1]\n",
            "108 + 9 = 117\n",
            "------------\n",
            "iters:7600\n",
            "Loss:0.005414480767292883\n",
            "Pred:[1 0 1 1 0 0 1 0]\n",
            "True:[1 0 1 1 0 0 1 0]\n",
            "120 + 58 = 178\n",
            "------------\n",
            "iters:7700\n",
            "Loss:0.002527789700722009\n",
            "Pred:[0 1 0 0 1 0 0 0]\n",
            "True:[0 1 0 0 1 0 0 0]\n",
            "32 + 40 = 72\n",
            "------------\n",
            "iters:7800\n",
            "Loss:0.003698652210301819\n",
            "Pred:[0 1 1 0 1 0 0 1]\n",
            "True:[0 1 1 0 1 0 0 1]\n",
            "56 + 49 = 105\n",
            "------------\n",
            "iters:7900\n",
            "Loss:0.005626404219656723\n",
            "Pred:[0 1 0 0 0 1 0 0]\n",
            "True:[0 1 0 0 0 1 0 0]\n",
            "14 + 54 = 68\n",
            "------------\n",
            "iters:8000\n",
            "Loss:0.0023353677968342076\n",
            "Pred:[0 1 0 0 1 1 1 0]\n",
            "True:[0 1 0 0 1 1 1 0]\n",
            "78 + 0 = 78\n",
            "------------\n",
            "iters:8100\n",
            "Loss:0.003404063880236301\n",
            "Pred:[1 0 0 0 1 1 0 1]\n",
            "True:[1 0 0 0 1 1 0 1]\n",
            "38 + 103 = 141\n",
            "------------\n",
            "iters:8200\n",
            "Loss:0.0051495757305939785\n",
            "Pred:[1 0 0 1 0 1 1 1]\n",
            "True:[1 0 0 1 0 1 1 1]\n",
            "93 + 58 = 151\n",
            "------------\n",
            "iters:8300\n",
            "Loss:0.00340068420094302\n",
            "Pred:[1 1 1 0 1 0 0 1]\n",
            "True:[1 1 1 0 1 0 0 1]\n",
            "119 + 114 = 233\n",
            "------------\n",
            "iters:8400\n",
            "Loss:0.0035044737661325243\n",
            "Pred:[0 1 1 0 0 1 0 1]\n",
            "True:[0 1 1 0 0 1 0 1]\n",
            "39 + 62 = 101\n",
            "------------\n",
            "iters:8500\n",
            "Loss:0.008688192873434596\n",
            "Pred:[1 0 0 1 1 1 1 0]\n",
            "True:[1 0 0 1 1 1 1 0]\n",
            "31 + 127 = 158\n",
            "------------\n",
            "iters:8600\n",
            "Loss:0.003882391500992129\n",
            "Pred:[0 1 1 1 0 0 0 0]\n",
            "True:[0 1 1 1 0 0 0 0]\n",
            "7 + 105 = 112\n",
            "------------\n",
            "iters:8700\n",
            "Loss:0.0011851843210785263\n",
            "Pred:[0 1 0 0 1 1 0 0]\n",
            "True:[0 1 0 0 1 1 0 0]\n",
            "11 + 65 = 76\n",
            "------------\n",
            "iters:8800\n",
            "Loss:0.0029258818845997437\n",
            "Pred:[0 1 1 1 1 1 0 1]\n",
            "True:[0 1 1 1 1 1 0 1]\n",
            "112 + 13 = 125\n",
            "------------\n",
            "iters:8900\n",
            "Loss:0.0009531942349929514\n",
            "Pred:[0 1 0 1 0 1 1 0]\n",
            "True:[0 1 0 1 0 1 1 0]\n",
            "85 + 1 = 86\n",
            "------------\n",
            "iters:9000\n",
            "Loss:0.0021926536691468292\n",
            "Pred:[0 1 1 0 1 0 0 1]\n",
            "True:[0 1 1 0 1 0 0 1]\n",
            "24 + 81 = 105\n",
            "------------\n",
            "iters:9100\n",
            "Loss:0.001679853051169449\n",
            "Pred:[0 1 1 1 1 0 0 1]\n",
            "True:[0 1 1 1 1 0 0 1]\n",
            "64 + 57 = 121\n",
            "------------\n",
            "iters:9200\n",
            "Loss:0.001921536118761196\n",
            "Pred:[0 0 1 0 0 1 0 1]\n",
            "True:[0 0 1 0 0 1 0 1]\n",
            "7 + 30 = 37\n",
            "------------\n",
            "iters:9300\n",
            "Loss:0.0016286952663089396\n",
            "Pred:[1 1 0 0 1 1 1 0]\n",
            "True:[1 1 0 0 1 1 1 0]\n",
            "104 + 102 = 206\n",
            "------------\n",
            "iters:9400\n",
            "Loss:0.0021442192069311993\n",
            "Pred:[1 0 0 0 1 1 0 0]\n",
            "True:[1 0 0 0 1 1 0 0]\n",
            "124 + 16 = 140\n",
            "------------\n",
            "iters:9500\n",
            "Loss:0.002198047302725894\n",
            "Pred:[0 1 0 1 1 0 0 0]\n",
            "True:[0 1 0 1 1 0 0 0]\n",
            "23 + 65 = 88\n",
            "------------\n",
            "iters:9600\n",
            "Loss:0.0013251103775957043\n",
            "Pred:[0 1 1 0 1 0 1 0]\n",
            "True:[0 1 1 0 1 0 1 0]\n",
            "106 + 0 = 106\n",
            "------------\n",
            "iters:9700\n",
            "Loss:0.0026571524918061525\n",
            "Pred:[1 0 1 0 0 1 0 0]\n",
            "True:[1 0 1 0 0 1 0 0]\n",
            "93 + 71 = 164\n",
            "------------\n",
            "iters:9800\n",
            "Loss:0.001999646707110067\n",
            "Pred:[1 0 1 0 0 0 1 1]\n",
            "True:[1 0 1 0 0 0 1 1]\n",
            "40 + 123 = 163\n",
            "------------\n",
            "iters:9900\n",
            "Loss:0.0014888590852794223\n",
            "Pred:[1 1 0 1 1 0 0 0]\n",
            "True:[1 1 0 1 1 0 0 0]\n",
            "98 + 118 = 216\n",
            "------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7zQEPrtP4OP"
      },
      "source": [
        "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "## [try] weight_init_stdやlearning_rate, hidden_layer_sizeを変更してみよう\n",
        "\n",
        "\n",
        "## [try] 重みの初期化方法を変更してみよう\n",
        "Xavier, He\n",
        "\n",
        "## [try] 中間層の活性化関数を変更してみよう\n",
        "ReLU(勾配爆発を確認しよう)<br>\n",
        "tanh(numpyにtanhが用意されている。導関数をd_tanhとして作成しよう)\n",
        "\n",
        "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSpLzNVuZAFm"
      },
      "source": [
        "**[try] weight_init_stdやlearning_rate, hidden_layer_sizeを変更してみよう**\n",
        "\n",
        "> hidden_layer_sizeを32にしてみる\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZeHK47EwZFto",
        "outputId": "9cbf977f-0040-4426-91cc-d6b8f43556cf"
      },
      "source": [
        "import numpy as np\n",
        "from common import functions\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# def d_tanh(x):\n",
        "\n",
        "\n",
        "\n",
        "# データを用意\n",
        "# 2進数の桁数\n",
        "binary_dim = 8\n",
        "# 最大値 + 1\n",
        "largest_number = pow(2, binary_dim)\n",
        "# largest_numberまで2進数を用意\n",
        "binary = np.unpackbits(np.array([range(largest_number)],dtype=np.uint8).T,axis=1)\n",
        "\n",
        "input_layer_size = 2\n",
        "hidden_layer_size = 32\n",
        "output_layer_size = 1\n",
        "\n",
        "weight_init_std = 1\n",
        "learning_rate = 0.1\n",
        "\n",
        "iters_num = 10000\n",
        "plot_interval = 100\n",
        "\n",
        "# ウェイト初期化 (バイアスは簡単のため省略)\n",
        "W_in = weight_init_std * np.random.randn(input_layer_size, hidden_layer_size)\n",
        "W_out = weight_init_std * np.random.randn(hidden_layer_size, output_layer_size)\n",
        "W = weight_init_std * np.random.randn(hidden_layer_size, hidden_layer_size)\n",
        "\n",
        "# Xavier\n",
        "\n",
        "\n",
        "# He\n",
        "\n",
        "\n",
        "\n",
        "# 勾配\n",
        "W_in_grad = np.zeros_like(W_in)\n",
        "W_out_grad = np.zeros_like(W_out)\n",
        "W_grad = np.zeros_like(W)\n",
        "\n",
        "u = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "z = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "y = np.zeros((output_layer_size, binary_dim))\n",
        "\n",
        "delta_out = np.zeros((output_layer_size, binary_dim))\n",
        "delta = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "\n",
        "all_losses = []\n",
        "\n",
        "for i in range(iters_num):\n",
        "    \n",
        "    # A, B初期化 (a + b = d)\n",
        "    a_int = np.random.randint(largest_number/2)\n",
        "    a_bin = binary[a_int] # binary encoding\n",
        "    b_int = np.random.randint(largest_number/2)\n",
        "    b_bin = binary[b_int] # binary encoding\n",
        "    \n",
        "    # 正解データ\n",
        "    d_int = a_int + b_int\n",
        "    d_bin = binary[d_int]\n",
        "    \n",
        "    # 出力バイナリ\n",
        "    out_bin = np.zeros_like(d_bin)\n",
        "    \n",
        "    # 時系列全体の誤差\n",
        "    all_loss = 0    \n",
        "    \n",
        "    # 時系列ループ\n",
        "    for t in range(binary_dim):\n",
        "        # 入力値\n",
        "        X = np.array([a_bin[ - t - 1], b_bin[ - t - 1]]).reshape(1, -1)\n",
        "        # 時刻tにおける正解データ\n",
        "        dd = np.array([d_bin[binary_dim - t - 1]])\n",
        "        \n",
        "        u[:,t+1] = np.dot(X, W_in) + np.dot(z[:,t].reshape(1, -1), W)\n",
        "        z[:,t+1] = functions.sigmoid(u[:,t+1])\n",
        "\n",
        "        y[:,t] = functions.sigmoid(np.dot(z[:,t+1].reshape(1, -1), W_out))\n",
        "\n",
        "\n",
        "        #誤差\n",
        "        loss = functions.mean_squared_error(dd, y[:,t])\n",
        "        \n",
        "        delta_out[:,t] = functions.d_mean_squared_error(dd, y[:,t]) * functions.d_sigmoid(y[:,t])        \n",
        "        \n",
        "        all_loss += loss\n",
        "\n",
        "        out_bin[binary_dim - t - 1] = np.round(y[:,t])\n",
        "    \n",
        "    \n",
        "    for t in range(binary_dim)[::-1]:\n",
        "        X = np.array([a_bin[-t-1],b_bin[-t-1]]).reshape(1, -1)        \n",
        "\n",
        "        delta[:,t] = (np.dot(delta[:,t+1].T, W.T) + np.dot(delta_out[:,t].T, W_out.T)) * functions.d_sigmoid(u[:,t+1])\n",
        "\n",
        "        # 勾配更新\n",
        "        W_out_grad += np.dot(z[:,t+1].reshape(-1,1), delta_out[:,t].reshape(-1,1))\n",
        "        W_grad += np.dot(z[:,t].reshape(-1,1), delta[:,t].reshape(1,-1))\n",
        "        W_in_grad += np.dot(X.T, delta[:,t].reshape(1,-1))\n",
        "    \n",
        "    # 勾配適用\n",
        "    W_in -= learning_rate * W_in_grad\n",
        "    W_out -= learning_rate * W_out_grad\n",
        "    W -= learning_rate * W_grad\n",
        "    \n",
        "    W_in_grad *= 0\n",
        "    W_out_grad *= 0\n",
        "    W_grad *= 0\n",
        "    \n",
        "\n",
        "    if(i % plot_interval == 0):\n",
        "        all_losses.append(all_loss)        \n",
        "        print(\"iters:\" + str(i))\n",
        "        print(\"Loss:\" + str(all_loss))\n",
        "        print(\"Pred:\" + str(out_bin))\n",
        "        print(\"True:\" + str(d_bin))\n",
        "        out_int = 0\n",
        "        for index,x in enumerate(reversed(out_bin)):\n",
        "            out_int += x * pow(2, index)\n",
        "        print(str(a_int) + \" + \" + str(b_int) + \" = \" + str(out_int))\n",
        "        print(\"------------\")\n",
        "\n",
        "lists = range(0, iters_num, plot_interval)\n",
        "plt.plot(lists, all_losses, label=\"loss\")\n",
        "plt.show()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iters:0\n",
            "Loss:1.4994310943928542\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 0 1 1 0 0 1 0]\n",
            "34 + 16 = 0\n",
            "------------\n",
            "iters:100\n",
            "Loss:1.0398018293743878\n",
            "Pred:[1 1 0 0 1 1 1 1]\n",
            "True:[0 1 1 0 0 0 1 1]\n",
            "28 + 71 = 207\n",
            "------------\n",
            "iters:200\n",
            "Loss:0.9680828809021211\n",
            "Pred:[0 0 0 0 0 0 0 1]\n",
            "True:[0 1 1 0 1 1 1 1]\n",
            "42 + 69 = 1\n",
            "------------\n",
            "iters:300\n",
            "Loss:1.1801844182239545\n",
            "Pred:[1 1 1 1 1 1 1 1]\n",
            "True:[0 0 1 0 1 0 0 1]\n",
            "8 + 33 = 255\n",
            "------------\n",
            "iters:400\n",
            "Loss:1.0019089966245862\n",
            "Pred:[1 1 1 1 0 1 1 0]\n",
            "True:[1 0 1 1 0 1 1 0]\n",
            "57 + 125 = 246\n",
            "------------\n",
            "iters:500\n",
            "Loss:1.048587005972284\n",
            "Pred:[1 1 1 1 1 1 1 0]\n",
            "True:[1 0 1 0 0 1 1 0]\n",
            "48 + 118 = 254\n",
            "------------\n",
            "iters:600\n",
            "Loss:0.6880390716431248\n",
            "Pred:[0 0 1 1 1 1 1 0]\n",
            "True:[0 0 0 1 1 1 1 0]\n",
            "8 + 22 = 62\n",
            "------------\n",
            "iters:700\n",
            "Loss:1.0799421667547597\n",
            "Pred:[0 1 1 1 1 1 1 1]\n",
            "True:[1 0 0 0 0 0 1 1]\n",
            "36 + 95 = 127\n",
            "------------\n",
            "iters:800\n",
            "Loss:0.7320471467766304\n",
            "Pred:[0 0 1 1 0 0 1 1]\n",
            "True:[0 0 1 0 0 1 1 1]\n",
            "15 + 24 = 51\n",
            "------------\n",
            "iters:900\n",
            "Loss:0.7730024819974566\n",
            "Pred:[1 0 0 1 0 0 0 0]\n",
            "True:[1 0 1 1 0 0 0 0]\n",
            "100 + 76 = 144\n",
            "------------\n",
            "iters:1000\n",
            "Loss:0.8465014990312104\n",
            "Pred:[1 1 1 1 1 1 1 1]\n",
            "True:[0 1 0 1 1 0 1 1]\n",
            "8 + 83 = 255\n",
            "------------\n",
            "iters:1100\n",
            "Loss:0.882378875718231\n",
            "Pred:[0 0 0 0 0 0 1 0]\n",
            "True:[0 1 1 0 0 0 1 0]\n",
            "85 + 13 = 2\n",
            "------------\n",
            "iters:1200\n",
            "Loss:0.6977284708766442\n",
            "Pred:[0 0 0 1 0 0 0 0]\n",
            "True:[0 1 0 1 0 0 1 0]\n",
            "8 + 74 = 16\n",
            "------------\n",
            "iters:1300\n",
            "Loss:0.6208482914726569\n",
            "Pred:[0 0 1 1 1 1 1 0]\n",
            "True:[0 0 1 1 1 0 1 0]\n",
            "20 + 38 = 62\n",
            "------------\n",
            "iters:1400\n",
            "Loss:0.6626387678752839\n",
            "Pred:[0 1 0 1 1 0 0 0]\n",
            "True:[0 1 0 1 1 1 0 0]\n",
            "55 + 37 = 88\n",
            "------------\n",
            "iters:1500\n",
            "Loss:0.514804249528089\n",
            "Pred:[0 0 1 0 1 0 0 0]\n",
            "True:[0 0 1 0 1 1 0 0]\n",
            "36 + 8 = 40\n",
            "------------\n",
            "iters:1600\n",
            "Loss:0.9416586495964396\n",
            "Pred:[1 1 1 0 1 0 1 1]\n",
            "True:[1 0 0 1 0 0 1 1]\n",
            "117 + 30 = 235\n",
            "------------\n",
            "iters:1700\n",
            "Loss:0.6081108316015865\n",
            "Pred:[0 0 0 1 0 1 1 1]\n",
            "True:[1 0 0 1 0 1 1 1]\n",
            "66 + 85 = 23\n",
            "------------\n",
            "iters:1800\n",
            "Loss:0.7611718677703817\n",
            "Pred:[0 0 1 0 0 0 0 1]\n",
            "True:[0 1 1 0 0 0 1 1]\n",
            "24 + 75 = 33\n",
            "------------\n",
            "iters:1900\n",
            "Loss:0.8600122943538706\n",
            "Pred:[1 0 1 1 1 1 1 1]\n",
            "True:[1 0 1 0 0 0 1 1]\n",
            "94 + 69 = 191\n",
            "------------\n",
            "iters:2000\n",
            "Loss:0.5820749417416147\n",
            "Pred:[1 0 0 1 0 0 1 1]\n",
            "True:[1 0 0 1 0 1 1 1]\n",
            "47 + 104 = 147\n",
            "------------\n",
            "iters:2100\n",
            "Loss:0.4279354234458797\n",
            "Pred:[1 1 1 1 1 0 0 1]\n",
            "True:[0 1 1 1 1 0 0 1]\n",
            "89 + 32 = 249\n",
            "------------\n",
            "iters:2200\n",
            "Loss:1.917950987678335\n",
            "Pred:[1 0 1 1 1 1 1 0]\n",
            "True:[1 1 0 0 0 0 0 0]\n",
            "95 + 97 = 190\n",
            "------------\n",
            "iters:2300\n",
            "Loss:0.8791248087830987\n",
            "Pred:[1 0 1 0 0 0 1 0]\n",
            "True:[0 1 1 1 0 0 1 0]\n",
            "29 + 85 = 162\n",
            "------------\n",
            "iters:2400\n",
            "Loss:1.0029853952792696\n",
            "Pred:[0 1 1 1 1 1 0 1]\n",
            "True:[0 1 0 1 0 0 0 1]\n",
            "61 + 20 = 125\n",
            "------------\n",
            "iters:2500\n",
            "Loss:0.3535692338504804\n",
            "Pred:[1 0 0 0 0 0 0 1]\n",
            "True:[1 0 0 0 0 0 0 1]\n",
            "45 + 84 = 129\n",
            "------------\n",
            "iters:2600\n",
            "Loss:0.09972724345231866\n",
            "Pred:[0 0 1 1 0 1 1 0]\n",
            "True:[0 0 1 1 0 1 1 0]\n",
            "36 + 18 = 54\n",
            "------------\n",
            "iters:2700\n",
            "Loss:0.38205207765995725\n",
            "Pred:[0 0 1 1 1 1 1 0]\n",
            "True:[0 0 1 1 1 0 1 0]\n",
            "55 + 3 = 62\n",
            "------------\n",
            "iters:2800\n",
            "Loss:0.4309662379042465\n",
            "Pred:[0 1 1 1 0 0 0 1]\n",
            "True:[0 1 1 1 1 0 0 1]\n",
            "29 + 92 = 113\n",
            "------------\n",
            "iters:2900\n",
            "Loss:0.8435092702785022\n",
            "Pred:[1 0 1 1 1 0 0 0]\n",
            "True:[1 0 0 0 0 0 0 0]\n",
            "57 + 71 = 184\n",
            "------------\n",
            "iters:3000\n",
            "Loss:0.647482959213862\n",
            "Pred:[1 0 0 0 0 0 0 1]\n",
            "True:[1 1 0 1 1 0 0 1]\n",
            "91 + 126 = 129\n",
            "------------\n",
            "iters:3100\n",
            "Loss:1.6466138886210309\n",
            "Pred:[1 1 0 0 1 1 0 1]\n",
            "True:[1 0 0 1 0 0 0 1]\n",
            "111 + 34 = 205\n",
            "------------\n",
            "iters:3200\n",
            "Loss:0.19807177568852435\n",
            "Pred:[0 0 0 1 1 0 0 0]\n",
            "True:[0 0 0 1 1 0 0 0]\n",
            "17 + 7 = 24\n",
            "------------\n",
            "iters:3300\n",
            "Loss:0.6566540155963289\n",
            "Pred:[0 1 1 0 1 1 0 0]\n",
            "True:[0 1 1 1 0 0 0 0]\n",
            "61 + 51 = 108\n",
            "------------\n",
            "iters:3400\n",
            "Loss:0.19503703986253135\n",
            "Pred:[1 1 0 0 0 1 1 0]\n",
            "True:[1 0 0 0 0 1 1 0]\n",
            "102 + 32 = 198\n",
            "------------\n",
            "iters:3500\n",
            "Loss:0.08208280300879149\n",
            "Pred:[1 0 0 1 1 0 1 1]\n",
            "True:[1 0 0 1 1 0 1 1]\n",
            "52 + 103 = 155\n",
            "------------\n",
            "iters:3600\n",
            "Loss:0.2392468144915955\n",
            "Pred:[1 0 1 0 1 1 0 0]\n",
            "True:[1 0 1 0 1 0 0 0]\n",
            "70 + 98 = 172\n",
            "------------\n",
            "iters:3700\n",
            "Loss:0.05372263408892848\n",
            "Pred:[0 0 0 0 1 1 0 1]\n",
            "True:[0 0 0 0 1 1 0 1]\n",
            "10 + 3 = 13\n",
            "------------\n",
            "iters:3800\n",
            "Loss:0.15598369636438755\n",
            "Pred:[0 0 1 0 0 1 0 0]\n",
            "True:[0 0 1 0 0 1 0 0]\n",
            "14 + 22 = 36\n",
            "------------\n",
            "iters:3900\n",
            "Loss:0.057744396186038736\n",
            "Pred:[1 0 0 1 1 0 0 1]\n",
            "True:[1 0 0 1 1 0 0 1]\n",
            "55 + 98 = 153\n",
            "------------\n",
            "iters:4000\n",
            "Loss:0.019015324755885164\n",
            "Pred:[0 1 0 1 0 0 0 1]\n",
            "True:[0 1 0 1 0 0 0 1]\n",
            "69 + 12 = 81\n",
            "------------\n",
            "iters:4100\n",
            "Loss:0.13430524531842697\n",
            "Pred:[0 1 1 0 1 0 0 0]\n",
            "True:[0 1 1 0 1 0 0 0]\n",
            "75 + 29 = 104\n",
            "------------\n",
            "iters:4200\n",
            "Loss:0.02522560811016442\n",
            "Pred:[0 1 1 0 1 1 1 0]\n",
            "True:[0 1 1 0 1 1 1 0]\n",
            "85 + 25 = 110\n",
            "------------\n",
            "iters:4300\n",
            "Loss:0.047620691925011976\n",
            "Pred:[0 1 1 0 0 1 0 0]\n",
            "True:[0 1 1 0 0 1 0 0]\n",
            "25 + 75 = 100\n",
            "------------\n",
            "iters:4400\n",
            "Loss:0.030479261242774697\n",
            "Pred:[0 1 0 0 1 0 0 1]\n",
            "True:[0 1 0 0 1 0 0 1]\n",
            "45 + 28 = 73\n",
            "------------\n",
            "iters:4500\n",
            "Loss:0.01620124745321766\n",
            "Pred:[1 0 0 0 0 0 1 0]\n",
            "True:[1 0 0 0 0 0 1 0]\n",
            "54 + 76 = 130\n",
            "------------\n",
            "iters:4600\n",
            "Loss:0.018775817080383124\n",
            "Pred:[0 1 0 1 0 0 1 0]\n",
            "True:[0 1 0 1 0 0 1 0]\n",
            "52 + 30 = 82\n",
            "------------\n",
            "iters:4700\n",
            "Loss:0.013603545852932742\n",
            "Pred:[0 1 0 0 0 0 1 1]\n",
            "True:[0 1 0 0 0 0 1 1]\n",
            "65 + 2 = 67\n",
            "------------\n",
            "iters:4800\n",
            "Loss:0.017292178851780037\n",
            "Pred:[0 1 0 1 1 0 1 1]\n",
            "True:[0 1 0 1 1 0 1 1]\n",
            "91 + 0 = 91\n",
            "------------\n",
            "iters:4900\n",
            "Loss:0.018199870221844206\n",
            "Pred:[0 1 1 0 0 0 1 1]\n",
            "True:[0 1 1 0 0 0 1 1]\n",
            "46 + 53 = 99\n",
            "------------\n",
            "iters:5000\n",
            "Loss:0.0069206056575622475\n",
            "Pred:[0 0 0 1 0 0 1 1]\n",
            "True:[0 0 0 1 0 0 1 1]\n",
            "18 + 1 = 19\n",
            "------------\n",
            "iters:5100\n",
            "Loss:0.014311085089471206\n",
            "Pred:[0 0 0 1 0 0 0 0]\n",
            "True:[0 0 0 1 0 0 0 0]\n",
            "12 + 4 = 16\n",
            "------------\n",
            "iters:5200\n",
            "Loss:0.004645069728118362\n",
            "Pred:[1 0 0 0 0 1 1 0]\n",
            "True:[1 0 0 0 0 1 1 0]\n",
            "113 + 21 = 134\n",
            "------------\n",
            "iters:5300\n",
            "Loss:0.01831188379906222\n",
            "Pred:[1 0 1 0 0 1 0 0]\n",
            "True:[1 0 1 0 0 1 0 0]\n",
            "93 + 71 = 164\n",
            "------------\n",
            "iters:5400\n",
            "Loss:0.014094719133396732\n",
            "Pred:[1 1 0 1 0 0 1 0]\n",
            "True:[1 1 0 1 0 0 1 0]\n",
            "92 + 118 = 210\n",
            "------------\n",
            "iters:5500\n",
            "Loss:0.007944707182230195\n",
            "Pred:[1 0 0 0 1 1 0 1]\n",
            "True:[1 0 0 0 1 1 0 1]\n",
            "22 + 119 = 141\n",
            "------------\n",
            "iters:5600\n",
            "Loss:0.010603113802765862\n",
            "Pred:[0 1 1 1 0 1 0 0]\n",
            "True:[0 1 1 1 0 1 0 0]\n",
            "90 + 26 = 116\n",
            "------------\n",
            "iters:5700\n",
            "Loss:0.008643932227236457\n",
            "Pred:[0 1 0 0 1 1 0 0]\n",
            "True:[0 1 0 0 1 1 0 0]\n",
            "50 + 26 = 76\n",
            "------------\n",
            "iters:5800\n",
            "Loss:0.0018679378184740338\n",
            "Pred:[1 0 1 1 1 0 1 0]\n",
            "True:[1 0 1 1 1 0 1 0]\n",
            "121 + 65 = 186\n",
            "------------\n",
            "iters:5900\n",
            "Loss:0.006322919482366097\n",
            "Pred:[1 1 1 0 0 1 0 1]\n",
            "True:[1 1 1 0 0 1 0 1]\n",
            "114 + 115 = 229\n",
            "------------\n",
            "iters:6000\n",
            "Loss:0.0064480290384976826\n",
            "Pred:[0 1 0 1 0 1 0 1]\n",
            "True:[0 1 0 1 0 1 0 1]\n",
            "14 + 71 = 85\n",
            "------------\n",
            "iters:6100\n",
            "Loss:0.007734987932023521\n",
            "Pred:[1 1 0 1 0 1 0 0]\n",
            "True:[1 1 0 1 0 1 0 0]\n",
            "125 + 87 = 212\n",
            "------------\n",
            "iters:6200\n",
            "Loss:0.006135245439082664\n",
            "Pred:[1 0 0 0 0 0 1 0]\n",
            "True:[1 0 0 0 0 0 1 0]\n",
            "120 + 10 = 130\n",
            "------------\n",
            "iters:6300\n",
            "Loss:0.0029313860914445106\n",
            "Pred:[0 1 1 1 1 1 1 0]\n",
            "True:[0 1 1 1 1 1 1 0]\n",
            "80 + 46 = 126\n",
            "------------\n",
            "iters:6400\n",
            "Loss:0.0069470693891218165\n",
            "Pred:[1 0 1 0 0 0 0 0]\n",
            "True:[1 0 1 0 0 0 0 0]\n",
            "95 + 65 = 160\n",
            "------------\n",
            "iters:6500\n",
            "Loss:0.0036562517832184407\n",
            "Pred:[0 1 1 1 1 1 1 1]\n",
            "True:[0 1 1 1 1 1 1 1]\n",
            "80 + 47 = 127\n",
            "------------\n",
            "iters:6600\n",
            "Loss:0.0025920430401389625\n",
            "Pred:[1 0 1 0 0 0 0 1]\n",
            "True:[1 0 1 0 0 0 0 1]\n",
            "40 + 121 = 161\n",
            "------------\n",
            "iters:6700\n",
            "Loss:0.0027268789534362287\n",
            "Pred:[1 0 1 1 1 1 1 1]\n",
            "True:[1 0 1 1 1 1 1 1]\n",
            "66 + 125 = 191\n",
            "------------\n",
            "iters:6800\n",
            "Loss:0.0037055710207793847\n",
            "Pred:[1 0 1 0 0 0 0 1]\n",
            "True:[1 0 1 0 0 0 0 1]\n",
            "43 + 118 = 161\n",
            "------------\n",
            "iters:6900\n",
            "Loss:0.004488131751689275\n",
            "Pred:[1 1 0 0 1 0 0 1]\n",
            "True:[1 1 0 0 1 0 0 1]\n",
            "91 + 110 = 201\n",
            "------------\n",
            "iters:7000\n",
            "Loss:0.006287672441915135\n",
            "Pred:[1 1 0 1 0 0 0 1]\n",
            "True:[1 1 0 1 0 0 0 1]\n",
            "95 + 114 = 209\n",
            "------------\n",
            "iters:7100\n",
            "Loss:0.004040296781266964\n",
            "Pred:[1 1 0 1 1 1 1 0]\n",
            "True:[1 1 0 1 1 1 1 0]\n",
            "95 + 127 = 222\n",
            "------------\n",
            "iters:7200\n",
            "Loss:0.002259471233133184\n",
            "Pred:[1 0 0 1 1 1 0 0]\n",
            "True:[1 0 0 1 1 1 0 0]\n",
            "50 + 106 = 156\n",
            "------------\n",
            "iters:7300\n",
            "Loss:0.005842541737686071\n",
            "Pred:[1 0 0 0 1 0 0 0]\n",
            "True:[1 0 0 0 1 0 0 0]\n",
            "59 + 77 = 136\n",
            "------------\n",
            "iters:7400\n",
            "Loss:0.0032245220887721854\n",
            "Pred:[1 1 1 1 0 0 1 0]\n",
            "True:[1 1 1 1 0 0 1 0]\n",
            "127 + 115 = 242\n",
            "------------\n",
            "iters:7500\n",
            "Loss:0.0025093128984648356\n",
            "Pred:[1 0 0 0 0 0 1 1]\n",
            "True:[1 0 0 0 0 0 1 1]\n",
            "85 + 46 = 131\n",
            "------------\n",
            "iters:7600\n",
            "Loss:0.0015739910480410885\n",
            "Pred:[0 1 0 1 0 1 0 0]\n",
            "True:[0 1 0 1 0 1 0 0]\n",
            "16 + 68 = 84\n",
            "------------\n",
            "iters:7700\n",
            "Loss:0.002050368039312952\n",
            "Pred:[0 1 0 0 0 0 0 1]\n",
            "True:[0 1 0 0 0 0 0 1]\n",
            "24 + 41 = 65\n",
            "------------\n",
            "iters:7800\n",
            "Loss:0.002708874771761946\n",
            "Pred:[1 0 0 0 0 0 0 1]\n",
            "True:[1 0 0 0 0 0 0 1]\n",
            "126 + 3 = 129\n",
            "------------\n",
            "iters:7900\n",
            "Loss:0.0025613727112290554\n",
            "Pred:[0 0 1 0 1 0 1 0]\n",
            "True:[0 0 1 0 1 0 1 0]\n",
            "28 + 14 = 42\n",
            "------------\n",
            "iters:8000\n",
            "Loss:0.0017071820898346322\n",
            "Pred:[0 1 1 0 1 0 0 0]\n",
            "True:[0 1 1 0 1 0 0 0]\n",
            "98 + 6 = 104\n",
            "------------\n",
            "iters:8100\n",
            "Loss:0.0014780395794000295\n",
            "Pred:[0 1 1 0 1 1 1 0]\n",
            "True:[0 1 1 0 1 1 1 0]\n",
            "8 + 102 = 110\n",
            "------------\n",
            "iters:8200\n",
            "Loss:0.0008897983150971645\n",
            "Pred:[0 0 1 0 1 1 0 1]\n",
            "True:[0 0 1 0 1 1 0 1]\n",
            "8 + 37 = 45\n",
            "------------\n",
            "iters:8300\n",
            "Loss:0.002256911311979102\n",
            "Pred:[1 0 1 0 1 0 1 0]\n",
            "True:[1 0 1 0 1 0 1 0]\n",
            "118 + 52 = 170\n",
            "------------\n",
            "iters:8400\n",
            "Loss:0.0018958286203140959\n",
            "Pred:[0 1 1 1 0 0 1 1]\n",
            "True:[0 1 1 1 0 0 1 1]\n",
            "61 + 54 = 115\n",
            "------------\n",
            "iters:8500\n",
            "Loss:0.00047865598113038834\n",
            "Pred:[0 1 1 0 0 1 1 0]\n",
            "True:[0 1 1 0 0 1 1 0]\n",
            "73 + 29 = 102\n",
            "------------\n",
            "iters:8600\n",
            "Loss:0.002049727824064127\n",
            "Pred:[1 1 1 0 0 0 1 1]\n",
            "True:[1 1 1 0 0 0 1 1]\n",
            "107 + 120 = 227\n",
            "------------\n",
            "iters:8700\n",
            "Loss:0.0011539675676215469\n",
            "Pred:[0 1 1 1 1 0 0 1]\n",
            "True:[0 1 1 1 1 0 0 1]\n",
            "98 + 23 = 121\n",
            "------------\n",
            "iters:8800\n",
            "Loss:0.001337944708358901\n",
            "Pred:[1 1 0 1 1 1 0 0]\n",
            "True:[1 1 0 1 1 1 0 0]\n",
            "119 + 101 = 220\n",
            "------------\n",
            "iters:8900\n",
            "Loss:0.0014654985423817456\n",
            "Pred:[1 0 0 0 1 1 1 0]\n",
            "True:[1 0 0 0 1 1 1 0]\n",
            "63 + 79 = 142\n",
            "------------\n",
            "iters:9000\n",
            "Loss:0.0013074669021614586\n",
            "Pred:[1 0 0 0 0 0 1 0]\n",
            "True:[1 0 0 0 0 0 1 0]\n",
            "44 + 86 = 130\n",
            "------------\n",
            "iters:9100\n",
            "Loss:0.0009673882009248769\n",
            "Pred:[0 1 1 1 1 0 1 0]\n",
            "True:[0 1 1 1 1 0 1 0]\n",
            "18 + 104 = 122\n",
            "------------\n",
            "iters:9200\n",
            "Loss:0.000986585673775432\n",
            "Pred:[1 1 0 1 0 0 0 0]\n",
            "True:[1 1 0 1 0 0 0 0]\n",
            "105 + 103 = 208\n",
            "------------\n",
            "iters:9300\n",
            "Loss:0.0011793775652164428\n",
            "Pred:[1 0 1 0 0 1 0 0]\n",
            "True:[1 0 1 0 0 1 0 0]\n",
            "96 + 68 = 164\n",
            "------------\n",
            "iters:9400\n",
            "Loss:0.0011342121152540473\n",
            "Pred:[0 0 1 0 1 1 0 0]\n",
            "True:[0 0 1 0 1 1 0 0]\n",
            "14 + 30 = 44\n",
            "------------\n",
            "iters:9500\n",
            "Loss:0.0005991591278296638\n",
            "Pred:[0 1 0 1 0 0 0 0]\n",
            "True:[0 1 0 1 0 0 0 0]\n",
            "9 + 71 = 80\n",
            "------------\n",
            "iters:9600\n",
            "Loss:0.001559576245062843\n",
            "Pred:[1 0 0 1 0 1 0 1]\n",
            "True:[1 0 0 1 0 1 0 1]\n",
            "31 + 118 = 149\n",
            "------------\n",
            "iters:9700\n",
            "Loss:0.0010636167210996618\n",
            "Pred:[0 1 0 1 0 1 0 1]\n",
            "True:[0 1 0 1 0 1 0 1]\n",
            "35 + 50 = 85\n",
            "------------\n",
            "iters:9800\n",
            "Loss:0.0003068359276797296\n",
            "Pred:[0 0 0 1 1 0 1 0]\n",
            "True:[0 0 0 1 1 0 1 0]\n",
            "13 + 13 = 26\n",
            "------------\n",
            "iters:9900\n",
            "Loss:0.0012869496151706413\n",
            "Pred:[1 1 0 0 0 1 0 0]\n",
            "True:[1 1 0 0 0 1 0 0]\n",
            "110 + 86 = 196\n",
            "------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMj6VKvcZxkI"
      },
      "source": [
        "**[try] 重みの初期化方法を変更してみよう**\n",
        "\n",
        "\n",
        "> 重みの初期化方法をHeにする\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3U3v_XFKaCU6",
        "outputId": "79dd5ea0-47e0-4601-8fb9-1370461cb660"
      },
      "source": [
        "import numpy as np\n",
        "from common import functions\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# def d_tanh(x):\n",
        "\n",
        "\n",
        "\n",
        "# データを用意\n",
        "# 2進数の桁数\n",
        "binary_dim = 8\n",
        "# 最大値 + 1\n",
        "largest_number = pow(2, binary_dim)\n",
        "# largest_numberまで2進数を用意\n",
        "binary = np.unpackbits(np.array([range(largest_number)],dtype=np.uint8).T,axis=1)\n",
        "\n",
        "input_layer_size = 2\n",
        "hidden_layer_size = 16\n",
        "output_layer_size = 1\n",
        "\n",
        "weight_init_std = 1\n",
        "learning_rate = 0.1\n",
        "\n",
        "iters_num = 10000\n",
        "plot_interval = 100\n",
        "\n",
        "# ウェイト初期化 (バイアスは簡単のため省略)\n",
        "W_in = weight_init_std * np.random.randn(input_layer_size, hidden_layer_size)\n",
        "W_out = weight_init_std * np.random.randn(hidden_layer_size, output_layer_size)\n",
        "W = weight_init_std * np.random.randn(hidden_layer_size, hidden_layer_size)\n",
        "\n",
        "# Xavier\n",
        "\n",
        "\n",
        "# He\n",
        "W_in = np.random.randn(input_layer_size, hidden_layer_size) / np.sqrt(input_layer_size) * np.sqrt(2)\n",
        "W_out = np.random.randn(hidden_layer_size, output_layer_size) / np.sqrt(hidden_layer_size) * np.sqrt(2)\n",
        "W = np.random.randn(hidden_layer_size, hidden_layer_size) / np.sqrt(hidden_layer_size) * np.sqrt(2)\n",
        "\n",
        "\n",
        "\n",
        "# 勾配\n",
        "W_in_grad = np.zeros_like(W_in)\n",
        "W_out_grad = np.zeros_like(W_out)\n",
        "W_grad = np.zeros_like(W)\n",
        "\n",
        "u = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "z = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "y = np.zeros((output_layer_size, binary_dim))\n",
        "\n",
        "delta_out = np.zeros((output_layer_size, binary_dim))\n",
        "delta = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "\n",
        "all_losses = []\n",
        "\n",
        "for i in range(iters_num):\n",
        "    \n",
        "    # A, B初期化 (a + b = d)\n",
        "    a_int = np.random.randint(largest_number/2)\n",
        "    a_bin = binary[a_int] # binary encoding\n",
        "    b_int = np.random.randint(largest_number/2)\n",
        "    b_bin = binary[b_int] # binary encoding\n",
        "    \n",
        "    # 正解データ\n",
        "    d_int = a_int + b_int\n",
        "    d_bin = binary[d_int]\n",
        "    \n",
        "    # 出力バイナリ\n",
        "    out_bin = np.zeros_like(d_bin)\n",
        "    \n",
        "    # 時系列全体の誤差\n",
        "    all_loss = 0    \n",
        "    \n",
        "    # 時系列ループ\n",
        "    for t in range(binary_dim):\n",
        "        # 入力値\n",
        "        X = np.array([a_bin[ - t - 1], b_bin[ - t - 1]]).reshape(1, -1)\n",
        "        # 時刻tにおける正解データ\n",
        "        dd = np.array([d_bin[binary_dim - t - 1]])\n",
        "        \n",
        "        u[:,t+1] = np.dot(X, W_in) + np.dot(z[:,t].reshape(1, -1), W)\n",
        "        z[:,t+1] = functions.sigmoid(u[:,t+1])\n",
        "\n",
        "        y[:,t] = functions.sigmoid(np.dot(z[:,t+1].reshape(1, -1), W_out))\n",
        "\n",
        "\n",
        "        #誤差\n",
        "        loss = functions.mean_squared_error(dd, y[:,t])\n",
        "        \n",
        "        delta_out[:,t] = functions.d_mean_squared_error(dd, y[:,t]) * functions.d_sigmoid(y[:,t])        \n",
        "        \n",
        "        all_loss += loss\n",
        "\n",
        "        out_bin[binary_dim - t - 1] = np.round(y[:,t])\n",
        "    \n",
        "    \n",
        "    for t in range(binary_dim)[::-1]:\n",
        "        X = np.array([a_bin[-t-1],b_bin[-t-1]]).reshape(1, -1)        \n",
        "\n",
        "        delta[:,t] = (np.dot(delta[:,t+1].T, W.T) + np.dot(delta_out[:,t].T, W_out.T)) * functions.d_sigmoid(u[:,t+1])\n",
        "\n",
        "        # 勾配更新\n",
        "        W_out_grad += np.dot(z[:,t+1].reshape(-1,1), delta_out[:,t].reshape(-1,1))\n",
        "        W_grad += np.dot(z[:,t].reshape(-1,1), delta[:,t].reshape(1,-1))\n",
        "        W_in_grad += np.dot(X.T, delta[:,t].reshape(1,-1))\n",
        "    \n",
        "    # 勾配適用\n",
        "    W_in -= learning_rate * W_in_grad\n",
        "    W_out -= learning_rate * W_out_grad\n",
        "    W -= learning_rate * W_grad\n",
        "    \n",
        "    W_in_grad *= 0\n",
        "    W_out_grad *= 0\n",
        "    W_grad *= 0\n",
        "    \n",
        "\n",
        "    if(i % plot_interval == 0):\n",
        "        all_losses.append(all_loss)        \n",
        "        print(\"iters:\" + str(i))\n",
        "        print(\"Loss:\" + str(all_loss))\n",
        "        print(\"Pred:\" + str(out_bin))\n",
        "        print(\"True:\" + str(d_bin))\n",
        "        out_int = 0\n",
        "        for index,x in enumerate(reversed(out_bin)):\n",
        "            out_int += x * pow(2, index)\n",
        "        print(str(a_int) + \" + \" + str(b_int) + \" = \" + str(out_int))\n",
        "        print(\"------------\")\n",
        "\n",
        "lists = range(0, iters_num, plot_interval)\n",
        "plt.plot(lists, all_losses, label=\"loss\")\n",
        "plt.show()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iters:0\n",
            "Loss:1.2877737604343562\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 1 0 1 1 1 0 0]\n",
            "122 + 98 = 0\n",
            "------------\n",
            "iters:100\n",
            "Loss:1.0135535614597575\n",
            "Pred:[1 1 1 1 1 1 1 1]\n",
            "True:[0 1 1 1 1 0 0 0]\n",
            "91 + 29 = 255\n",
            "------------\n",
            "iters:200\n",
            "Loss:1.0544820291233217\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 1 0 0 0 1 1 0]\n",
            "125 + 73 = 0\n",
            "------------\n",
            "iters:300\n",
            "Loss:0.9604685647147775\n",
            "Pred:[0 0 1 0 1 0 1 0]\n",
            "True:[0 0 1 1 1 0 0 0]\n",
            "21 + 35 = 42\n",
            "------------\n",
            "iters:400\n",
            "Loss:0.9019969909978888\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 0 0 1 0 0]\n",
            "36 + 32 = 0\n",
            "------------\n",
            "iters:500\n",
            "Loss:0.981585161755708\n",
            "Pred:[0 0 0 1 1 0 1 1]\n",
            "True:[1 0 0 0 1 0 1 1]\n",
            "108 + 31 = 27\n",
            "------------\n",
            "iters:600\n",
            "Loss:1.0462223077443435\n",
            "Pred:[0 1 1 1 0 1 0 0]\n",
            "True:[1 0 0 0 0 1 0 0]\n",
            "122 + 10 = 116\n",
            "------------\n",
            "iters:700\n",
            "Loss:0.9353742557546866\n",
            "Pred:[0 1 0 0 0 0 0 0]\n",
            "True:[0 1 1 1 1 0 1 0]\n",
            "12 + 110 = 64\n",
            "------------\n",
            "iters:800\n",
            "Loss:1.0707835591854904\n",
            "Pred:[0 0 1 1 1 0 0 0]\n",
            "True:[1 0 0 0 1 0 1 1]\n",
            "95 + 44 = 56\n",
            "------------\n",
            "iters:900\n",
            "Loss:0.9278475006974434\n",
            "Pred:[1 1 0 0 1 1 1 1]\n",
            "True:[0 1 1 0 0 1 1 1]\n",
            "103 + 0 = 207\n",
            "------------\n",
            "iters:1000\n",
            "Loss:1.0602587118457192\n",
            "Pred:[0 1 0 0 0 1 0 0]\n",
            "True:[0 1 1 1 0 0 0 1]\n",
            "11 + 102 = 68\n",
            "------------\n",
            "iters:1100\n",
            "Loss:1.1283367375141766\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 0 1 1 1 1 1 1]\n",
            "38 + 25 = 0\n",
            "------------\n",
            "iters:1200\n",
            "Loss:0.6473787751465163\n",
            "Pred:[0 0 1 1 1 1 1 1]\n",
            "True:[0 0 1 0 1 1 1 1]\n",
            "17 + 30 = 63\n",
            "------------\n",
            "iters:1300\n",
            "Loss:0.785754790787521\n",
            "Pred:[0 1 1 1 1 1 1 1]\n",
            "True:[0 1 1 0 0 1 1 1]\n",
            "44 + 59 = 127\n",
            "------------\n",
            "iters:1400\n",
            "Loss:0.7919679769994646\n",
            "Pred:[0 1 0 1 1 1 1 1]\n",
            "True:[0 1 0 1 1 1 1 1]\n",
            "31 + 64 = 95\n",
            "------------\n",
            "iters:1500\n",
            "Loss:0.7783384461438814\n",
            "Pred:[1 0 0 0 0 1 0 1]\n",
            "True:[1 0 0 1 0 1 0 1]\n",
            "69 + 80 = 133\n",
            "------------\n",
            "iters:1600\n",
            "Loss:0.6315766719365057\n",
            "Pred:[0 1 0 0 0 1 0 1]\n",
            "True:[0 1 1 0 0 1 0 1]\n",
            "50 + 51 = 69\n",
            "------------\n",
            "iters:1700\n",
            "Loss:0.8691388836236696\n",
            "Pred:[0 1 0 0 0 1 1 0]\n",
            "True:[0 1 1 1 0 1 0 0]\n",
            "35 + 81 = 70\n",
            "------------\n",
            "iters:1800\n",
            "Loss:0.647691730030321\n",
            "Pred:[1 1 1 1 1 1 1 1]\n",
            "True:[1 1 0 1 1 1 1 0]\n",
            "99 + 123 = 255\n",
            "------------\n",
            "iters:1900\n",
            "Loss:0.4943324185634333\n",
            "Pred:[0 0 1 0 0 0 0 1]\n",
            "True:[0 0 1 0 1 0 0 1]\n",
            "25 + 16 = 33\n",
            "------------\n",
            "iters:2000\n",
            "Loss:0.4587202987636366\n",
            "Pred:[0 1 1 1 0 0 0 0]\n",
            "True:[0 1 1 1 0 0 0 0]\n",
            "72 + 40 = 112\n",
            "------------\n",
            "iters:2100\n",
            "Loss:0.5423714223945386\n",
            "Pred:[1 0 0 0 0 1 1 1]\n",
            "True:[1 0 1 0 0 1 1 1]\n",
            "66 + 101 = 135\n",
            "------------\n",
            "iters:2200\n",
            "Loss:1.0437794853452935\n",
            "Pred:[1 1 0 1 1 0 1 1]\n",
            "True:[1 1 1 0 0 0 1 1]\n",
            "103 + 124 = 219\n",
            "------------\n",
            "iters:2300\n",
            "Loss:1.3455352737204096\n",
            "Pred:[0 1 1 1 0 1 1 0]\n",
            "True:[1 0 0 0 1 0 0 0]\n",
            "77 + 59 = 118\n",
            "------------\n",
            "iters:2400\n",
            "Loss:0.6836270585124352\n",
            "Pred:[1 1 0 0 0 1 1 1]\n",
            "True:[1 1 1 0 0 0 1 1]\n",
            "113 + 114 = 199\n",
            "------------\n",
            "iters:2500\n",
            "Loss:0.3643444717772611\n",
            "Pred:[0 1 0 1 1 1 1 1]\n",
            "True:[0 1 0 1 1 1 1 1]\n",
            "32 + 63 = 95\n",
            "------------\n",
            "iters:2600\n",
            "Loss:0.4952820745356523\n",
            "Pred:[0 1 1 0 1 1 1 1]\n",
            "True:[0 1 1 0 1 1 0 1]\n",
            "81 + 28 = 111\n",
            "------------\n",
            "iters:2700\n",
            "Loss:0.3889055844010353\n",
            "Pred:[1 1 0 1 0 0 0 0]\n",
            "True:[1 0 0 1 0 0 0 0]\n",
            "48 + 96 = 208\n",
            "------------\n",
            "iters:2800\n",
            "Loss:0.22840812310732772\n",
            "Pred:[1 0 1 1 1 1 1 0]\n",
            "True:[1 0 1 1 1 1 1 0]\n",
            "88 + 102 = 190\n",
            "------------\n",
            "iters:2900\n",
            "Loss:0.45934452404910864\n",
            "Pred:[1 0 0 1 1 1 1 0]\n",
            "True:[1 0 0 1 1 1 1 0]\n",
            "92 + 66 = 158\n",
            "------------\n",
            "iters:3000\n",
            "Loss:0.15737715251772735\n",
            "Pred:[0 1 1 1 1 0 1 0]\n",
            "True:[0 1 1 1 1 0 1 0]\n",
            "93 + 29 = 122\n",
            "------------\n",
            "iters:3100\n",
            "Loss:0.15232007910389633\n",
            "Pred:[0 1 1 0 1 0 0 1]\n",
            "True:[0 1 1 0 1 0 0 1]\n",
            "104 + 1 = 105\n",
            "------------\n",
            "iters:3200\n",
            "Loss:0.17451338077362294\n",
            "Pred:[1 0 1 1 1 0 0 1]\n",
            "True:[1 0 1 1 1 0 0 1]\n",
            "64 + 121 = 185\n",
            "------------\n",
            "iters:3300\n",
            "Loss:0.22992328899347264\n",
            "Pred:[1 0 1 0 0 1 0 0]\n",
            "True:[1 0 1 0 0 1 0 0]\n",
            "51 + 113 = 164\n",
            "------------\n",
            "iters:3400\n",
            "Loss:0.34594902908306563\n",
            "Pred:[0 1 1 1 0 0 1 0]\n",
            "True:[0 1 1 0 0 0 1 0]\n",
            "28 + 70 = 114\n",
            "------------\n",
            "iters:3500\n",
            "Loss:0.23778889174094378\n",
            "Pred:[0 1 0 1 1 0 1 0]\n",
            "True:[0 1 0 1 0 0 1 0]\n",
            "11 + 71 = 90\n",
            "------------\n",
            "iters:3600\n",
            "Loss:0.22771471915532435\n",
            "Pred:[0 1 1 1 0 0 1 1]\n",
            "True:[0 1 1 1 0 0 1 1]\n",
            "69 + 46 = 115\n",
            "------------\n",
            "iters:3700\n",
            "Loss:0.12829796546275252\n",
            "Pred:[0 0 1 1 0 0 0 0]\n",
            "True:[0 0 1 1 0 0 0 0]\n",
            "12 + 36 = 48\n",
            "------------\n",
            "iters:3800\n",
            "Loss:0.09883313995216685\n",
            "Pred:[1 0 1 0 0 1 1 1]\n",
            "True:[1 0 1 0 0 1 1 1]\n",
            "71 + 96 = 167\n",
            "------------\n",
            "iters:3900\n",
            "Loss:0.0520106449172453\n",
            "Pred:[0 1 1 1 1 1 0 0]\n",
            "True:[0 1 1 1 1 1 0 0]\n",
            "121 + 3 = 124\n",
            "------------\n",
            "iters:4000\n",
            "Loss:0.032506389500027176\n",
            "Pred:[0 1 1 0 1 1 0 0]\n",
            "True:[0 1 1 0 1 1 0 0]\n",
            "39 + 69 = 108\n",
            "------------\n",
            "iters:4100\n",
            "Loss:0.31872822682585444\n",
            "Pred:[0 0 1 1 1 0 1 1]\n",
            "True:[0 0 1 1 1 0 1 1]\n",
            "6 + 53 = 59\n",
            "------------\n",
            "iters:4200\n",
            "Loss:0.057370325418085164\n",
            "Pred:[1 0 1 0 0 1 1 0]\n",
            "True:[1 0 1 0 0 1 1 0]\n",
            "63 + 103 = 166\n",
            "------------\n",
            "iters:4300\n",
            "Loss:0.022393473776451157\n",
            "Pred:[0 1 1 0 0 1 1 0]\n",
            "True:[0 1 1 0 0 1 1 0]\n",
            "45 + 57 = 102\n",
            "------------\n",
            "iters:4400\n",
            "Loss:0.030590377180781093\n",
            "Pred:[0 0 0 1 1 1 0 1]\n",
            "True:[0 0 0 1 1 1 0 1]\n",
            "28 + 1 = 29\n",
            "------------\n",
            "iters:4500\n",
            "Loss:0.04585961625661806\n",
            "Pred:[1 0 0 0 1 0 1 0]\n",
            "True:[1 0 0 0 1 0 1 0]\n",
            "46 + 92 = 138\n",
            "------------\n",
            "iters:4600\n",
            "Loss:0.01511076424079798\n",
            "Pred:[1 1 0 0 1 0 0 1]\n",
            "True:[1 1 0 0 1 0 0 1]\n",
            "116 + 85 = 201\n",
            "------------\n",
            "iters:4700\n",
            "Loss:0.02681046126993133\n",
            "Pred:[1 0 1 0 0 1 1 0]\n",
            "True:[1 0 1 0 0 1 1 0]\n",
            "63 + 103 = 166\n",
            "------------\n",
            "iters:4800\n",
            "Loss:0.03880627005624691\n",
            "Pred:[1 0 0 1 0 1 1 1]\n",
            "True:[1 0 0 1 0 1 1 1]\n",
            "91 + 60 = 151\n",
            "------------\n",
            "iters:4900\n",
            "Loss:0.017793331892472147\n",
            "Pred:[0 1 1 0 0 0 1 1]\n",
            "True:[0 1 1 0 0 0 1 1]\n",
            "80 + 19 = 99\n",
            "------------\n",
            "iters:5000\n",
            "Loss:0.020420311571168\n",
            "Pred:[0 1 1 1 0 0 0 0]\n",
            "True:[0 1 1 1 0 0 0 0]\n",
            "63 + 49 = 112\n",
            "------------\n",
            "iters:5100\n",
            "Loss:0.024524087246719672\n",
            "Pred:[0 1 1 1 0 0 0 1]\n",
            "True:[0 1 1 1 0 0 0 1]\n",
            "32 + 81 = 113\n",
            "------------\n",
            "iters:5200\n",
            "Loss:0.020643370096144507\n",
            "Pred:[0 0 0 1 1 0 0 0]\n",
            "True:[0 0 0 1 1 0 0 0]\n",
            "16 + 8 = 24\n",
            "------------\n",
            "iters:5300\n",
            "Loss:0.012213180454021718\n",
            "Pred:[0 1 1 0 1 1 1 1]\n",
            "True:[0 1 1 0 1 1 1 1]\n",
            "77 + 34 = 111\n",
            "------------\n",
            "iters:5400\n",
            "Loss:0.028368570210153297\n",
            "Pred:[0 1 1 1 0 1 0 0]\n",
            "True:[0 1 1 1 0 1 0 0]\n",
            "30 + 86 = 116\n",
            "------------\n",
            "iters:5500\n",
            "Loss:0.029504675625784942\n",
            "Pred:[1 1 1 0 0 0 0 1]\n",
            "True:[1 1 1 0 0 0 0 1]\n",
            "99 + 126 = 225\n",
            "------------\n",
            "iters:5600\n",
            "Loss:0.013960563176241104\n",
            "Pred:[0 1 0 0 1 0 1 0]\n",
            "True:[0 1 0 0 1 0 1 0]\n",
            "50 + 24 = 74\n",
            "------------\n",
            "iters:5700\n",
            "Loss:0.007922876464317566\n",
            "Pred:[1 0 1 0 0 1 1 0]\n",
            "True:[1 0 1 0 0 1 1 0]\n",
            "55 + 111 = 166\n",
            "------------\n",
            "iters:5800\n",
            "Loss:0.007805824770492928\n",
            "Pred:[1 1 0 0 1 0 0 1]\n",
            "True:[1 1 0 0 1 0 0 1]\n",
            "103 + 98 = 201\n",
            "------------\n",
            "iters:5900\n",
            "Loss:0.011644483159569216\n",
            "Pred:[0 0 1 0 1 0 1 1]\n",
            "True:[0 0 1 0 1 0 1 1]\n",
            "39 + 4 = 43\n",
            "------------\n",
            "iters:6000\n",
            "Loss:0.010201212646923644\n",
            "Pred:[0 1 0 0 0 0 0 1]\n",
            "True:[0 1 0 0 0 0 0 1]\n",
            "13 + 52 = 65\n",
            "------------\n",
            "iters:6100\n",
            "Loss:0.011001145183785898\n",
            "Pred:[0 1 1 0 1 0 1 1]\n",
            "True:[0 1 1 0 1 0 1 1]\n",
            "51 + 56 = 107\n",
            "------------\n",
            "iters:6200\n",
            "Loss:0.01169622160263981\n",
            "Pred:[0 1 0 1 0 0 1 0]\n",
            "True:[0 1 0 1 0 0 1 0]\n",
            "50 + 32 = 82\n",
            "------------\n",
            "iters:6300\n",
            "Loss:0.040359401496191966\n",
            "Pred:[1 0 0 0 0 0 1 0]\n",
            "True:[1 0 0 0 0 0 1 0]\n",
            "86 + 44 = 130\n",
            "------------\n",
            "iters:6400\n",
            "Loss:0.005441712555419837\n",
            "Pred:[1 1 1 0 0 0 1 1]\n",
            "True:[1 1 1 0 0 0 1 1]\n",
            "126 + 101 = 227\n",
            "------------\n",
            "iters:6500\n",
            "Loss:0.0071784113191513165\n",
            "Pred:[1 0 1 1 0 1 1 1]\n",
            "True:[1 0 1 1 0 1 1 1]\n",
            "109 + 74 = 183\n",
            "------------\n",
            "iters:6600\n",
            "Loss:0.013385335713195433\n",
            "Pred:[1 0 0 0 1 0 0 0]\n",
            "True:[1 0 0 0 1 0 0 0]\n",
            "18 + 118 = 136\n",
            "------------\n",
            "iters:6700\n",
            "Loss:0.005238900109251539\n",
            "Pred:[1 0 1 0 0 0 1 1]\n",
            "True:[1 0 1 0 0 0 1 1]\n",
            "84 + 79 = 163\n",
            "------------\n",
            "iters:6800\n",
            "Loss:0.007695058452496609\n",
            "Pred:[1 1 0 1 0 0 1 0]\n",
            "True:[1 1 0 1 0 0 1 0]\n",
            "112 + 98 = 210\n",
            "------------\n",
            "iters:6900\n",
            "Loss:0.008294390193089123\n",
            "Pred:[1 0 0 1 0 1 0 0]\n",
            "True:[1 0 0 1 0 1 0 0]\n",
            "88 + 60 = 148\n",
            "------------\n",
            "iters:7000\n",
            "Loss:0.005054718722347418\n",
            "Pred:[0 1 1 0 1 1 1 1]\n",
            "True:[0 1 1 0 1 1 1 1]\n",
            "105 + 6 = 111\n",
            "------------\n",
            "iters:7100\n",
            "Loss:0.003145634668085935\n",
            "Pred:[0 1 1 0 1 1 1 1]\n",
            "True:[0 1 1 0 1 1 1 1]\n",
            "14 + 97 = 111\n",
            "------------\n",
            "iters:7200\n",
            "Loss:0.025754491076575833\n",
            "Pred:[0 1 1 0 0 0 1 0]\n",
            "True:[0 1 1 0 0 0 1 0]\n",
            "16 + 82 = 98\n",
            "------------\n",
            "iters:7300\n",
            "Loss:0.003347201455496414\n",
            "Pred:[0 1 0 1 1 0 0 1]\n",
            "True:[0 1 0 1 1 0 0 1]\n",
            "71 + 18 = 89\n",
            "------------\n",
            "iters:7400\n",
            "Loss:0.0037785673484684464\n",
            "Pred:[1 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 0 0 0 0 0]\n",
            "47 + 81 = 128\n",
            "------------\n",
            "iters:7500\n",
            "Loss:0.008181125943767383\n",
            "Pred:[1 0 0 0 1 1 1 0]\n",
            "True:[1 0 0 0 1 1 1 0]\n",
            "22 + 120 = 142\n",
            "------------\n",
            "iters:7600\n",
            "Loss:0.0035226942276484798\n",
            "Pred:[1 0 0 1 1 0 1 1]\n",
            "True:[1 0 0 1 1 0 1 1]\n",
            "81 + 74 = 155\n",
            "------------\n",
            "iters:7700\n",
            "Loss:0.0017817091855383655\n",
            "Pred:[1 0 1 0 0 0 0 0]\n",
            "True:[1 0 1 0 0 0 0 0]\n",
            "69 + 91 = 160\n",
            "------------\n",
            "iters:7800\n",
            "Loss:0.00020039507879786664\n",
            "Pred:[0 1 1 0 1 0 1 0]\n",
            "True:[0 1 1 0 1 0 1 0]\n",
            "97 + 9 = 106\n",
            "------------\n",
            "iters:7900\n",
            "Loss:0.0019739628430098495\n",
            "Pred:[0 1 0 0 1 0 0 1]\n",
            "True:[0 1 0 0 1 0 0 1]\n",
            "44 + 29 = 73\n",
            "------------\n",
            "iters:8000\n",
            "Loss:0.0032062080043339138\n",
            "Pred:[1 0 0 0 1 0 1 1]\n",
            "True:[1 0 0 0 1 0 1 1]\n",
            "78 + 61 = 139\n",
            "------------\n",
            "iters:8100\n",
            "Loss:0.00285283827109108\n",
            "Pred:[0 0 1 0 0 1 1 1]\n",
            "True:[0 0 1 0 0 1 1 1]\n",
            "21 + 18 = 39\n",
            "------------\n",
            "iters:8200\n",
            "Loss:0.00091432655284916\n",
            "Pred:[1 0 1 0 0 1 1 0]\n",
            "True:[1 0 1 0 0 1 1 0]\n",
            "55 + 111 = 166\n",
            "------------\n",
            "iters:8300\n",
            "Loss:0.0022674405017108577\n",
            "Pred:[0 1 1 1 1 1 1 1]\n",
            "True:[0 1 1 1 1 1 1 1]\n",
            "62 + 65 = 127\n",
            "------------\n",
            "iters:8400\n",
            "Loss:0.0004950223541068363\n",
            "Pred:[1 1 0 0 0 1 0 0]\n",
            "True:[1 1 0 0 0 1 0 0]\n",
            "123 + 73 = 196\n",
            "------------\n",
            "iters:8500\n",
            "Loss:0.0005091787089155307\n",
            "Pred:[0 0 1 0 0 1 1 0]\n",
            "True:[0 0 1 0 0 1 1 0]\n",
            "15 + 23 = 38\n",
            "------------\n",
            "iters:8600\n",
            "Loss:0.0013434677025435716\n",
            "Pred:[0 1 1 1 0 1 0 1]\n",
            "True:[0 1 1 1 0 1 0 1]\n",
            "50 + 67 = 117\n",
            "------------\n",
            "iters:8700\n",
            "Loss:0.0011555866604128818\n",
            "Pred:[0 0 1 1 1 1 0 1]\n",
            "True:[0 0 1 1 1 1 0 1]\n",
            "22 + 39 = 61\n",
            "------------\n",
            "iters:8800\n",
            "Loss:0.000370061764677819\n",
            "Pred:[0 1 0 0 0 0 1 0]\n",
            "True:[0 1 0 0 0 0 1 0]\n",
            "41 + 25 = 66\n",
            "------------\n",
            "iters:8900\n",
            "Loss:0.0008314837170944273\n",
            "Pred:[1 1 1 1 1 0 0 0]\n",
            "True:[1 1 1 1 1 0 0 0]\n",
            "121 + 127 = 248\n",
            "------------\n",
            "iters:9000\n",
            "Loss:0.0002512875066035257\n",
            "Pred:[0 1 1 1 0 0 1 0]\n",
            "True:[0 1 1 1 0 0 1 0]\n",
            "41 + 73 = 114\n",
            "------------\n",
            "iters:9100\n",
            "Loss:0.0016568550538235318\n",
            "Pred:[1 0 0 0 0 1 1 1]\n",
            "True:[1 0 0 0 0 1 1 1]\n",
            "48 + 87 = 135\n",
            "------------\n",
            "iters:9200\n",
            "Loss:0.0031887986909998204\n",
            "Pred:[1 0 0 1 1 1 1 0]\n",
            "True:[1 0 0 1 1 1 1 0]\n",
            "34 + 124 = 158\n",
            "------------\n",
            "iters:9300\n",
            "Loss:0.003160807314079973\n",
            "Pred:[0 1 0 1 0 0 1 0]\n",
            "True:[0 1 0 1 0 0 1 0]\n",
            "82 + 0 = 82\n",
            "------------\n",
            "iters:9400\n",
            "Loss:0.0021917004720947097\n",
            "Pred:[1 1 1 0 0 1 0 1]\n",
            "True:[1 1 1 0 0 1 0 1]\n",
            "103 + 126 = 229\n",
            "------------\n",
            "iters:9500\n",
            "Loss:0.0002634570210010664\n",
            "Pred:[0 1 0 0 1 1 1 0]\n",
            "True:[0 1 0 0 1 1 1 0]\n",
            "17 + 61 = 78\n",
            "------------\n",
            "iters:9600\n",
            "Loss:0.0018422970668911657\n",
            "Pred:[1 0 1 0 0 0 1 1]\n",
            "True:[1 0 1 0 0 0 1 1]\n",
            "63 + 100 = 163\n",
            "------------\n",
            "iters:9700\n",
            "Loss:0.0015931444945513108\n",
            "Pred:[1 0 1 0 1 0 0 1]\n",
            "True:[1 0 1 0 1 0 0 1]\n",
            "49 + 120 = 169\n",
            "------------\n",
            "iters:9800\n",
            "Loss:0.0002503858042191723\n",
            "Pred:[1 0 1 0 1 0 1 0]\n",
            "True:[1 0 1 0 1 0 1 0]\n",
            "115 + 55 = 170\n",
            "------------\n",
            "iters:9900\n",
            "Loss:0.0006218882826985173\n",
            "Pred:[1 0 1 0 0 0 0 0]\n",
            "True:[1 0 1 0 0 0 0 0]\n",
            "89 + 71 = 160\n",
            "------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53tcqvY6aQCY"
      },
      "source": [
        "**[try] 中間層の活性化関数を変更してみよう**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Uxj0Kr1TaXsF",
        "outputId": "e1c6fca9-eb04-46fc-eab8-905a840999ab"
      },
      "source": [
        "import numpy as np\n",
        "from common import functions\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# def d_tanh(x):\n",
        "\n",
        "\n",
        "\n",
        "# データを用意\n",
        "# 2進数の桁数\n",
        "binary_dim = 8\n",
        "# 最大値 + 1\n",
        "largest_number = pow(2, binary_dim)\n",
        "# largest_numberまで2進数を用意\n",
        "binary = np.unpackbits(np.array([range(largest_number)],dtype=np.uint8).T,axis=1)\n",
        "\n",
        "input_layer_size = 2\n",
        "hidden_layer_size = 32\n",
        "output_layer_size = 1\n",
        "\n",
        "weight_init_std = 1\n",
        "learning_rate = 0.1\n",
        "\n",
        "iters_num = 10000\n",
        "plot_interval = 100\n",
        "\n",
        "# ウェイト初期化 (バイアスは簡単のため省略)\n",
        "W_in = weight_init_std * np.random.randn(input_layer_size, hidden_layer_size)\n",
        "W_out = weight_init_std * np.random.randn(hidden_layer_size, output_layer_size)\n",
        "W = weight_init_std * np.random.randn(hidden_layer_size, hidden_layer_size)\n",
        "\n",
        "# Xavier\n",
        "\n",
        "\n",
        "# He\n",
        "\n",
        "\n",
        "\n",
        "# 勾配\n",
        "W_in_grad = np.zeros_like(W_in)\n",
        "W_out_grad = np.zeros_like(W_out)\n",
        "W_grad = np.zeros_like(W)\n",
        "\n",
        "u = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "z = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "y = np.zeros((output_layer_size, binary_dim))\n",
        "\n",
        "delta_out = np.zeros((output_layer_size, binary_dim))\n",
        "delta = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "\n",
        "all_losses = []\n",
        "\n",
        "for i in range(iters_num):\n",
        "    \n",
        "    # A, B初期化 (a + b = d)\n",
        "    a_int = np.random.randint(largest_number/2)\n",
        "    a_bin = binary[a_int] # binary encoding\n",
        "    b_int = np.random.randint(largest_number/2)\n",
        "    b_bin = binary[b_int] # binary encoding\n",
        "    \n",
        "    # 正解データ\n",
        "    d_int = a_int + b_int\n",
        "    d_bin = binary[d_int]\n",
        "    \n",
        "    # 出力バイナリ\n",
        "    out_bin = np.zeros_like(d_bin)\n",
        "    \n",
        "    # 時系列全体の誤差\n",
        "    all_loss = 0    \n",
        "    \n",
        "    # 時系列ループ\n",
        "    for t in range(binary_dim):\n",
        "        # 入力値\n",
        "        X = np.array([a_bin[ - t - 1], b_bin[ - t - 1]]).reshape(1, -1)\n",
        "        # 時刻tにおける正解データ\n",
        "        dd = np.array([d_bin[binary_dim - t - 1]])\n",
        "        \n",
        "        u[:,t+1] = np.dot(X, W_in) + np.dot(z[:,t].reshape(1, -1), W)\n",
        "        z[:,t+1] = functions.relu(u[:,t+1])\n",
        "\n",
        "        y[:,t] = functions.relu(np.dot(z[:,t+1].reshape(1, -1), W_out))\n",
        "\n",
        "\n",
        "        #誤差\n",
        "        loss = functions.mean_squared_error(dd, y[:,t])\n",
        "        \n",
        "        delta_out[:,t] = functions.d_mean_squared_error(dd, y[:,t]) * functions.d_relu(y[:,t])        \n",
        "        \n",
        "        all_loss += loss\n",
        "\n",
        "        out_bin[binary_dim - t - 1] = np.round(y[:,t])\n",
        "    \n",
        "    \n",
        "    for t in range(binary_dim)[::-1]:\n",
        "        X = np.array([a_bin[-t-1],b_bin[-t-1]]).reshape(1, -1)        \n",
        "\n",
        "        delta[:,t] = (np.dot(delta[:,t+1].T, W.T) + np.dot(delta_out[:,t].T, W_out.T)) * functions.d_relu(u[:,t+1])\n",
        "\n",
        "        # 勾配更新\n",
        "        W_out_grad += np.dot(z[:,t+1].reshape(-1,1), delta_out[:,t].reshape(-1,1))\n",
        "        W_grad += np.dot(z[:,t].reshape(-1,1), delta[:,t].reshape(1,-1))\n",
        "        W_in_grad += np.dot(X.T, delta[:,t].reshape(1,-1))\n",
        "    \n",
        "    # 勾配適用\n",
        "    W_in -= learning_rate * W_in_grad\n",
        "    W_out -= learning_rate * W_out_grad\n",
        "    W -= learning_rate * W_grad\n",
        "    \n",
        "    W_in_grad *= 0\n",
        "    W_out_grad *= 0\n",
        "    W_grad *= 0\n",
        "    \n",
        "\n",
        "    if(i % plot_interval == 0):\n",
        "        all_losses.append(all_loss)        \n",
        "        print(\"iters:\" + str(i))\n",
        "        print(\"Loss:\" + str(all_loss))\n",
        "        print(\"Pred:\" + str(out_bin))\n",
        "        print(\"True:\" + str(d_bin))\n",
        "        out_int = 0\n",
        "        for index,x in enumerate(reversed(out_bin)):\n",
        "            out_int += x * pow(2, index)\n",
        "        print(str(a_int) + \" + \" + str(b_int) + \" = \" + str(out_int))\n",
        "        print(\"------------\")\n",
        "\n",
        "lists = range(0, iters_num, plot_interval)\n",
        "plt.plot(lists, all_losses, label=\"loss\")\n",
        "plt.show()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iters:0\n",
            "Loss:25032518.89620119\n",
            "Pred:[  0   0 153 136  47   0   0   7]\n",
            "True:[0 1 0 0 0 1 1 1]\n",
            "58 + 13 = 7455\n",
            "------------\n",
            "iters:100\n",
            "Loss:1.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 0 0 0 0 1]\n",
            "104 + 57 = 0\n",
            "------------\n",
            "iters:200\n",
            "Loss:2.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 0 1 1 0 1]\n",
            "21 + 88 = 0\n",
            "------------\n",
            "iters:300\n",
            "Loss:1.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 0 1 1 0 0 1 0]\n",
            "13 + 37 = 0\n",
            "------------\n",
            "iters:400\n",
            "Loss:2.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 1 1 0 0 1]\n",
            "114 + 39 = 0\n",
            "------------\n",
            "iters:500\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 0 1 0 0 0 0 1]\n",
            "3 + 30 = 0\n",
            "------------\n",
            "iters:600\n",
            "Loss:1.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 0 1 0 0 1]\n",
            "20 + 53 = 0\n",
            "------------\n",
            "iters:700\n",
            "Loss:2.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 1 1 1 0 0]\n",
            "74 + 18 = 0\n",
            "------------\n",
            "iters:800\n",
            "Loss:1.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 1 0 1 0 0 0 0]\n",
            "86 + 122 = 0\n",
            "------------\n",
            "iters:900\n",
            "Loss:1.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 0 1 0 0 1]\n",
            "105 + 32 = 0\n",
            "------------\n",
            "iters:1000\n",
            "Loss:1.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 0 0 0 0 1]\n",
            "91 + 6 = 0\n",
            "------------\n",
            "iters:1100\n",
            "Loss:3.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 1 1 1 1 1]\n",
            "81 + 14 = 0\n",
            "------------\n",
            "iters:1200\n",
            "Loss:2.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 1 1 1 0 0]\n",
            "118 + 38 = 0\n",
            "------------\n",
            "iters:1300\n",
            "Loss:2.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 1 1 0 0 0 1 1]\n",
            "101 + 126 = 0\n",
            "------------\n",
            "iters:1400\n",
            "Loss:3.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 1 1 1 1 1]\n",
            "24 + 71 = 0\n",
            "------------\n",
            "iters:1500\n",
            "Loss:3.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 1 1 1 1 1]\n",
            "53 + 42 = 0\n",
            "------------\n",
            "iters:1600\n",
            "Loss:2.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 0 0 1 1 1]\n",
            "20 + 83 = 0\n",
            "------------\n",
            "iters:1700\n",
            "Loss:2.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 0 0 0 1 1]\n",
            "92 + 7 = 0\n",
            "------------\n",
            "iters:1800\n",
            "Loss:2.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 1 0 1 1 1]\n",
            "109 + 42 = 0\n",
            "------------\n",
            "iters:1900\n",
            "Loss:2.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 1 0 1 0 1]\n",
            "29 + 120 = 0\n",
            "------------\n",
            "iters:2000\n",
            "Loss:2.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 1 0 1 0 0]\n",
            "93 + 23 = 0\n",
            "------------\n",
            "iters:2100\n",
            "Loss:2.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 1 1 1 1 0]\n",
            "59 + 99 = 0\n",
            "------------\n",
            "iters:2200\n",
            "Loss:2.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 1 1 0 0 1 0 0]\n",
            "103 + 125 = 0\n",
            "------------\n",
            "iters:2300\n",
            "Loss:1.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 0 0 1 0 1]\n",
            "121 + 12 = 0\n",
            "------------\n",
            "iters:2400\n",
            "Loss:2.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 0 1 0 1 0]\n",
            "124 + 46 = 0\n",
            "------------\n",
            "iters:2500\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 1 0 0 0 0 0 0]\n",
            "103 + 89 = 0\n",
            "------------\n",
            "iters:2600\n",
            "Loss:1.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 0 1 0 0 0]\n",
            "73 + 31 = 0\n",
            "------------\n",
            "iters:2700\n",
            "Loss:2.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 1 1 0 1 0 0 0]\n",
            "125 + 107 = 0\n",
            "------------\n",
            "iters:2800\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 0 0 0 0 1]\n",
            "56 + 73 = 0\n",
            "------------\n",
            "iters:2900\n",
            "Loss:2.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 0 1 1 1 1 0 0]\n",
            "9 + 51 = 0\n",
            "------------\n",
            "iters:3000\n",
            "Loss:2.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 1 0 1 0 0]\n",
            "73 + 43 = 0\n",
            "------------\n",
            "iters:3100\n",
            "Loss:3.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 1 1 1 1 0]\n",
            "115 + 11 = 0\n",
            "------------\n",
            "iters:3200\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 0 0 0 0 1]\n",
            "62 + 3 = 0\n",
            "------------\n",
            "iters:3300\n",
            "Loss:2.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 1 1 0 0 1]\n",
            "90 + 31 = 0\n",
            "------------\n",
            "iters:3400\n",
            "Loss:2.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 1 0 0 0 1]\n",
            "8 + 105 = 0\n",
            "------------\n",
            "iters:3500\n",
            "Loss:2.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 1 0 1 1 0]\n",
            "115 + 3 = 0\n",
            "------------\n",
            "iters:3600\n",
            "Loss:2.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 1 1 1 1 0]\n",
            "66 + 92 = 0\n",
            "------------\n",
            "iters:3700\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 0 0 0 0 1]\n",
            "36 + 93 = 0\n",
            "------------\n",
            "iters:3800\n",
            "Loss:2.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 0 0 1 0 1]\n",
            "23 + 78 = 0\n",
            "------------\n",
            "iters:3900\n",
            "Loss:2.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 1 0 0 1 1]\n",
            "38 + 109 = 0\n",
            "------------\n",
            "iters:4000\n",
            "Loss:2.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 1 0 0 0 0 1 1]\n",
            "79 + 116 = 0\n",
            "------------\n",
            "iters:4100\n",
            "Loss:1.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 1 0 1 0 0 0 0]\n",
            "126 + 82 = 0\n",
            "------------\n",
            "iters:4200\n",
            "Loss:2.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 1 0 1 0 1 1 0]\n",
            "111 + 103 = 0\n",
            "------------\n",
            "iters:4300\n",
            "Loss:2.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 1 1 0 1 1]\n",
            "87 + 4 = 0\n",
            "------------\n",
            "iters:4400\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 0 0 0 0 0]\n",
            "91 + 69 = 0\n",
            "------------\n",
            "iters:4500\n",
            "Loss:2.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 0 1 1 0 1]\n",
            "6 + 71 = 0\n",
            "------------\n",
            "iters:4600\n",
            "Loss:2.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 1 1 1 0 0]\n",
            "86 + 102 = 0\n",
            "------------\n",
            "iters:4700\n",
            "Loss:2.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 1 1 1 0 0]\n",
            "73 + 83 = 0\n",
            "------------\n",
            "iters:4800\n",
            "Loss:2.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 1 0 1 0 1]\n",
            "58 + 59 = 0\n",
            "------------\n",
            "iters:4900\n",
            "Loss:2.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 0 1 1 1 0]\n",
            "74 + 68 = 0\n",
            "------------\n",
            "iters:5000\n",
            "Loss:2.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 1 0 0 1 1]\n",
            "49 + 34 = 0\n",
            "------------\n",
            "iters:5100\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 0 0 0 1 0]\n",
            "43 + 23 = 0\n",
            "------------\n",
            "iters:5200\n",
            "Loss:3.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 1 0 1 1 1 1 1]\n",
            "98 + 125 = 0\n",
            "------------\n",
            "iters:5300\n",
            "Loss:2.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 1 1 0 0 1]\n",
            "6 + 83 = 0\n",
            "------------\n",
            "iters:5400\n",
            "Loss:2.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 1 1 0 0 1 0 1]\n",
            "122 + 107 = 0\n",
            "------------\n",
            "iters:5500\n",
            "Loss:1.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 1 1 0 0 0]\n",
            "47 + 41 = 0\n",
            "------------\n",
            "iters:5600\n",
            "Loss:2.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 1 1 1 0 0]\n",
            "109 + 79 = 0\n",
            "------------\n",
            "iters:5700\n",
            "Loss:1.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 1 0 0 0 1]\n",
            "14 + 67 = 0\n",
            "------------\n",
            "iters:5800\n",
            "Loss:2.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 1 0 0 1 1]\n",
            "61 + 22 = 0\n",
            "------------\n",
            "iters:5900\n",
            "Loss:3.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 0 1 1 1 1]\n",
            "56 + 55 = 0\n",
            "------------\n",
            "iters:6000\n",
            "Loss:2.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 0 0 0 1 1]\n",
            "74 + 25 = 0\n",
            "------------\n",
            "iters:6100\n",
            "Loss:3.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 1 1 1 1 1]\n",
            "90 + 101 = 0\n",
            "------------\n",
            "iters:6200\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 0 0 1 0 0 1 0]\n",
            "11 + 7 = 0\n",
            "------------\n",
            "iters:6300\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 1 0 0 0 0]\n",
            "27 + 53 = 0\n",
            "------------\n",
            "iters:6400\n",
            "Loss:3.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 1 1 1 1 1]\n",
            "110 + 49 = 0\n",
            "------------\n",
            "iters:6500\n",
            "Loss:2.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 0 1 1 1 1]\n",
            "77 + 66 = 0\n",
            "------------\n",
            "iters:6600\n",
            "Loss:2.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 1 0 1 1 1]\n",
            "106 + 45 = 0\n",
            "------------\n",
            "iters:6700\n",
            "Loss:2.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 1 1 0 1 0]\n",
            "16 + 106 = 0\n",
            "------------\n",
            "iters:6800\n",
            "Loss:1.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 0 0 1 0 1]\n",
            "67 + 66 = 0\n",
            "------------\n",
            "iters:6900\n",
            "Loss:1.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 0 1 1 0 0 1 0]\n",
            "46 + 4 = 0\n",
            "------------\n",
            "iters:7000\n",
            "Loss:2.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 1 0 1 1 0 0 1]\n",
            "117 + 100 = 0\n",
            "------------\n",
            "iters:7100\n",
            "Loss:2.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 1 0 1 1 1]\n",
            "15 + 72 = 0\n",
            "------------\n",
            "iters:7200\n",
            "Loss:2.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 1 0 0 1 1]\n",
            "54 + 125 = 0\n",
            "------------\n",
            "iters:7300\n",
            "Loss:2.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 1 0 1 0 1]\n",
            "38 + 47 = 0\n",
            "------------\n",
            "iters:7400\n",
            "Loss:1.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 1 0 1 0 0]\n",
            "88 + 60 = 0\n",
            "------------\n",
            "iters:7500\n",
            "Loss:2.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 0 1 0 0 1]\n",
            "33 + 72 = 0\n",
            "------------\n",
            "iters:7600\n",
            "Loss:2.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 1 0 0 1 1]\n",
            "82 + 1 = 0\n",
            "------------\n",
            "iters:7700\n",
            "Loss:2.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 1 1 0 0 1]\n",
            "107 + 14 = 0\n",
            "------------\n",
            "iters:7800\n",
            "Loss:1.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 1 1 0 0 0]\n",
            "60 + 28 = 0\n",
            "------------\n",
            "iters:7900\n",
            "Loss:2.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 1 1 1 0 0]\n",
            "101 + 55 = 0\n",
            "------------\n",
            "iters:8000\n",
            "Loss:2.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 1 1 0 0 1]\n",
            "104 + 17 = 0\n",
            "------------\n",
            "iters:8100\n",
            "Loss:2.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 0 1 1 0 1]\n",
            "118 + 55 = 0\n",
            "------------\n",
            "iters:8200\n",
            "Loss:1.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 0 1 0 0 0 1 1]\n",
            "34 + 1 = 0\n",
            "------------\n",
            "iters:8300\n",
            "Loss:2.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 1 1 1 0 0]\n",
            "52 + 40 = 0\n",
            "------------\n",
            "iters:8400\n",
            "Loss:1.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 1 0 0 0 1 0 0]\n",
            "124 + 72 = 0\n",
            "------------\n",
            "iters:8500\n",
            "Loss:2.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 1 0 0 0 1]\n",
            "50 + 127 = 0\n",
            "------------\n",
            "iters:8600\n",
            "Loss:2.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 0 1 1 0 0]\n",
            "101 + 71 = 0\n",
            "------------\n",
            "iters:8700\n",
            "Loss:2.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 1 0 1 1 0]\n",
            "66 + 20 = 0\n",
            "------------\n",
            "iters:8800\n",
            "Loss:1.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 0 1 0 0 0]\n",
            "20 + 84 = 0\n",
            "------------\n",
            "iters:8900\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 0 0 0 1 0]\n",
            "6 + 60 = 0\n",
            "------------\n",
            "iters:9000\n",
            "Loss:2.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 1 0 0 1 0 0 1]\n",
            "81 + 120 = 0\n",
            "------------\n",
            "iters:9100\n",
            "Loss:1.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 0 1 0 0 0]\n",
            "4 + 100 = 0\n",
            "------------\n",
            "iters:9200\n",
            "Loss:2.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 1 1 0 1 0]\n",
            "118 + 68 = 0\n",
            "------------\n",
            "iters:9300\n",
            "Loss:2.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 0 1 1 0 0]\n",
            "69 + 39 = 0\n",
            "------------\n",
            "iters:9400\n",
            "Loss:2.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 1 1 0 0 0]\n",
            "76 + 44 = 0\n",
            "------------\n",
            "iters:9500\n",
            "Loss:3.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 0 1 1 1 1]\n",
            "24 + 87 = 0\n",
            "------------\n",
            "iters:9600\n",
            "Loss:2.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 0 1 0 0 1]\n",
            "18 + 87 = 0\n",
            "------------\n",
            "iters:9700\n",
            "Loss:2.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 1 1 0 1 0 0 0]\n",
            "116 + 116 = 0\n",
            "------------\n",
            "iters:9800\n",
            "Loss:1.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 1 0 0 1 0]\n",
            "17 + 65 = 0\n",
            "------------\n",
            "iters:9900\n",
            "Loss:1.5\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 0 1 0 0 0]\n",
            "65 + 103 = 0\n",
            "------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}