{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "name": "3_1_simple_RNN_after.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KoHa2/DeepLearning/blob/main/RabbitCharange/Stage4/3_1_simple_RNN_after.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cNl2QA_Rnv5"
      },
      "source": [
        "# 準備"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkwjN1jNVAYy"
      },
      "source": [
        "## Googleドライブのマウント"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvFXpiH3EVC1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad6bf188-4879-4d10-ea77-826356ccbe23"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ub7RYdeY6pK"
      },
      "source": [
        "## sys.pathの設定"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oql7L19rEsWi"
      },
      "source": [
        "以下では，Googleドライブのマイドライブ直下にDNN_codeフォルダを置くことを仮定しています．必要に応じて，パスを変更してください．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ic2JzkvFX59"
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/E/04.stage4/DNN_code_colab_lesson_3_4')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzGmsHRwO-bi"
      },
      "source": [
        "# simple RNN after\n",
        "### バイナリ加算"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "KNSG0aKXO-bk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "58122700-a869-4ce4-fbf2-f1106b059ae1"
      },
      "source": [
        "import numpy as np\n",
        "from common import functions\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def d_tanh(x):\n",
        "    return 1/(np.cosh(x) ** 2)\n",
        "\n",
        "# データを用意\n",
        "# 2進数の桁数\n",
        "binary_dim = 8\n",
        "# 最大値 + 1\n",
        "largest_number = pow(2, binary_dim)\n",
        "# largest_numberまで2進数を用意\n",
        "binary = np.unpackbits(np.array([range(largest_number)],dtype=np.uint8).T,axis=1)\n",
        "\n",
        "input_layer_size = 2\n",
        "hidden_layer_size = 16\n",
        "output_layer_size = 1\n",
        "\n",
        "weight_init_std = 1\n",
        "learning_rate = 0.1\n",
        "\n",
        "iters_num = 10000\n",
        "plot_interval = 100\n",
        "\n",
        "# ウェイト初期化 (バイアスは簡単のため省略)\n",
        "W_in = weight_init_std * np.random.randn(input_layer_size, hidden_layer_size)\n",
        "W_out = weight_init_std * np.random.randn(hidden_layer_size, output_layer_size)\n",
        "W = weight_init_std * np.random.randn(hidden_layer_size, hidden_layer_size)\n",
        "# Xavier\n",
        "# W_in = np.random.randn(input_layer_size, hidden_layer_size) / (np.sqrt(input_layer_size))\n",
        "# W_out = np.random.randn(hidden_layer_size, output_layer_size) / (np.sqrt(hidden_layer_size))\n",
        "# W = np.random.randn(hidden_layer_size, hidden_layer_size) / (np.sqrt(hidden_layer_size))\n",
        "# He\n",
        "# W_in = np.random.randn(input_layer_size, hidden_layer_size) / (np.sqrt(input_layer_size)) * np.sqrt(2)\n",
        "# W_out = np.random.randn(hidden_layer_size, output_layer_size) / (np.sqrt(hidden_layer_size)) * np.sqrt(2)\n",
        "# W = np.random.randn(hidden_layer_size, hidden_layer_size) / (np.sqrt(hidden_layer_size)) * np.sqrt(2)\n",
        "\n",
        "\n",
        "# 勾配\n",
        "W_in_grad = np.zeros_like(W_in)\n",
        "W_out_grad = np.zeros_like(W_out)\n",
        "W_grad = np.zeros_like(W)\n",
        "\n",
        "u = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "z = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "y = np.zeros((output_layer_size, binary_dim))\n",
        "\n",
        "delta_out = np.zeros((output_layer_size, binary_dim))\n",
        "delta = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "\n",
        "all_losses = []\n",
        "\n",
        "for i in range(iters_num):\n",
        "    \n",
        "    # A, B初期化 (a + b = d)\n",
        "    a_int = np.random.randint(largest_number/2)\n",
        "    a_bin = binary[a_int] # binary encoding\n",
        "    b_int = np.random.randint(largest_number/2)\n",
        "    b_bin = binary[b_int] # binary encoding\n",
        "    \n",
        "    # 正解データ\n",
        "    d_int = a_int + b_int\n",
        "    d_bin = binary[d_int]\n",
        "    \n",
        "    # 出力バイナリ\n",
        "    out_bin = np.zeros_like(d_bin)\n",
        "    \n",
        "    # 時系列全体の誤差\n",
        "    all_loss = 0    \n",
        "    \n",
        "    # 時系列ループ\n",
        "    for t in range(binary_dim):\n",
        "        # 入力値\n",
        "        X = np.array([a_bin[ - t - 1], b_bin[ - t - 1]]).reshape(1, -1)\n",
        "        # 時刻tにおける正解データ\n",
        "        dd = np.array([d_bin[binary_dim - t - 1]])\n",
        "        \n",
        "        u[:,t+1] = np.dot(X, W_in) + np.dot(z[:,t].reshape(1, -1), W)\n",
        "        z[:,t+1] = functions.sigmoid(u[:,t+1])\n",
        "#         z[:,t+1] = functions.relu(u[:,t+1])\n",
        "#         z[:,t+1] = np.tanh(u[:,t+1])    \n",
        "        y[:,t] = functions.sigmoid(np.dot(z[:,t+1].reshape(1, -1), W_out))\n",
        "\n",
        "\n",
        "        #誤差\n",
        "        loss = functions.mean_squared_error(dd, y[:,t])\n",
        "        \n",
        "        delta_out[:,t] = functions.d_mean_squared_error(dd, y[:,t]) * functions.d_sigmoid(y[:,t])        \n",
        "        \n",
        "        all_loss += loss\n",
        "\n",
        "        out_bin[binary_dim - t - 1] = np.round(y[:,t])\n",
        "    \n",
        "    \n",
        "    for t in range(binary_dim)[::-1]:\n",
        "        X = np.array([a_bin[-t-1],b_bin[-t-1]]).reshape(1, -1)        \n",
        "\n",
        "        delta[:,t] = (np.dot(delta[:,t+1].T, W.T) + np.dot(delta_out[:,t].T, W_out.T)) * functions.d_sigmoid(u[:,t+1])\n",
        "#         delta[:,t] = (np.dot(delta[:,t+1].T, W.T) + np.dot(delta_out[:,t].T, W_out.T)) * functions.d_relu(u[:,t+1])\n",
        "#         delta[:,t] = (np.dot(delta[:,t+1].T, W.T) + np.dot(delta_out[:,t].T, W_out.T)) * d_tanh(u[:,t+1])    \n",
        "\n",
        "        # 勾配更新\n",
        "        W_out_grad += np.dot(z[:,t+1].reshape(-1,1), delta_out[:,t].reshape(-1,1))\n",
        "        W_grad += np.dot(z[:,t].reshape(-1,1), delta[:,t].reshape(1,-1))\n",
        "        W_in_grad += np.dot(X.T, delta[:,t].reshape(1,-1))\n",
        "    \n",
        "    # 勾配適用\n",
        "    W_in -= learning_rate * W_in_grad\n",
        "    W_out -= learning_rate * W_out_grad\n",
        "    W -= learning_rate * W_grad\n",
        "    \n",
        "    W_in_grad *= 0\n",
        "    W_out_grad *= 0\n",
        "    W_grad *= 0\n",
        "    \n",
        "\n",
        "    if(i % plot_interval == 0):\n",
        "        all_losses.append(all_loss)        \n",
        "        print(\"iters:\" + str(i))\n",
        "        print(\"Loss:\" + str(all_loss))\n",
        "        print(\"Pred:\" + str(out_bin))\n",
        "        print(\"True:\" + str(d_bin))\n",
        "        out_int = 0\n",
        "        for index,x in enumerate(reversed(out_bin)):\n",
        "            out_int += x * pow(2, index)\n",
        "        print(str(a_int) + \" + \" + str(b_int) + \" = \" + str(out_int))\n",
        "        print(\"------------\")\n",
        "\n",
        "lists = range(0, iters_num, plot_interval)\n",
        "plt.plot(lists, all_losses, label=\"loss\")\n",
        "plt.show()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iters:0\n",
            "Loss:1.8800826816984035\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 1 0 1 1 0]\n",
            "76 + 74 = 0\n",
            "------------\n",
            "iters:100\n",
            "Loss:1.0604207056337123\n",
            "Pred:[0 1 1 1 0 0 0 1]\n",
            "True:[0 1 1 0 0 0 1 0]\n",
            "49 + 49 = 113\n",
            "------------\n",
            "iters:200\n",
            "Loss:1.0266195504598175\n",
            "Pred:[0 1 0 1 1 1 0 0]\n",
            "True:[0 1 0 1 0 0 1 1]\n",
            "42 + 41 = 92\n",
            "------------\n",
            "iters:300\n",
            "Loss:1.0514895786620775\n",
            "Pred:[0 0 1 1 1 0 0 1]\n",
            "True:[1 0 0 1 0 1 1 1]\n",
            "28 + 123 = 57\n",
            "------------\n",
            "iters:400\n",
            "Loss:1.0538990913196622\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 0 0 1 1 1]\n",
            "84 + 51 = 0\n",
            "------------\n",
            "iters:500\n",
            "Loss:0.8589891606906696\n",
            "Pred:[1 0 1 1 0 1 0 0]\n",
            "True:[1 0 1 1 1 0 0 0]\n",
            "94 + 90 = 180\n",
            "------------\n",
            "iters:600\n",
            "Loss:1.0107460495212486\n",
            "Pred:[0 0 0 1 1 1 0 0]\n",
            "True:[1 0 0 0 0 0 0 1]\n",
            "30 + 99 = 28\n",
            "------------\n",
            "iters:700\n",
            "Loss:0.9736462719882613\n",
            "Pred:[0 1 1 0 1 0 1 0]\n",
            "True:[0 0 1 1 1 1 0 0]\n",
            "41 + 19 = 106\n",
            "------------\n",
            "iters:800\n",
            "Loss:1.0647672835770279\n",
            "Pred:[1 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 1 1 1 0 0]\n",
            "66 + 122 = 128\n",
            "------------\n",
            "iters:900\n",
            "Loss:0.9224404006955177\n",
            "Pred:[1 0 1 1 0 0 1 1]\n",
            "True:[1 0 1 1 1 1 1 1]\n",
            "117 + 74 = 179\n",
            "------------\n",
            "iters:1000\n",
            "Loss:0.7777399304704513\n",
            "Pred:[0 1 1 0 0 0 0 0]\n",
            "True:[0 1 1 1 0 0 0 0]\n",
            "57 + 55 = 96\n",
            "------------\n",
            "iters:1100\n",
            "Loss:1.0518513309793456\n",
            "Pred:[0 1 0 0 1 1 1 0]\n",
            "True:[1 0 0 0 0 0 0 0]\n",
            "75 + 53 = 78\n",
            "------------\n",
            "iters:1200\n",
            "Loss:0.9171858260309936\n",
            "Pred:[0 0 0 0 0 1 1 0]\n",
            "True:[1 0 0 0 0 0 0 0]\n",
            "7 + 121 = 6\n",
            "------------\n",
            "iters:1300\n",
            "Loss:1.0617346774416587\n",
            "Pred:[0 1 0 1 0 0 0 0]\n",
            "True:[1 0 0 1 0 1 1 0]\n",
            "106 + 44 = 80\n",
            "------------\n",
            "iters:1400\n",
            "Loss:0.7792335726739357\n",
            "Pred:[1 1 1 1 0 1 1 0]\n",
            "True:[0 1 1 1 1 1 1 0]\n",
            "19 + 107 = 246\n",
            "------------\n",
            "iters:1500\n",
            "Loss:0.9171115223768949\n",
            "Pred:[0 0 0 1 0 1 0 0]\n",
            "True:[0 1 0 0 0 1 1 0]\n",
            "12 + 58 = 20\n",
            "------------\n",
            "iters:1600\n",
            "Loss:0.7165028727422118\n",
            "Pred:[0 0 1 1 1 1 1 0]\n",
            "True:[0 0 1 1 1 1 1 1]\n",
            "62 + 1 = 62\n",
            "------------\n",
            "iters:1700\n",
            "Loss:0.9855175573688408\n",
            "Pred:[1 1 1 1 1 1 1 0]\n",
            "True:[1 0 1 1 1 0 0 0]\n",
            "83 + 101 = 254\n",
            "------------\n",
            "iters:1800\n",
            "Loss:0.5474976214790197\n",
            "Pred:[1 0 0 0 0 1 1 1]\n",
            "True:[1 0 0 0 0 1 1 1]\n",
            "71 + 64 = 135\n",
            "------------\n",
            "iters:1900\n",
            "Loss:0.7914691693725551\n",
            "Pred:[0 1 0 0 0 1 0 0]\n",
            "True:[0 1 0 0 0 1 1 1]\n",
            "20 + 51 = 68\n",
            "------------\n",
            "iters:2000\n",
            "Loss:0.8235317470969514\n",
            "Pred:[0 0 0 0 1 1 1 1]\n",
            "True:[0 1 0 1 0 1 1 1]\n",
            "86 + 1 = 15\n",
            "------------\n",
            "iters:2100\n",
            "Loss:0.47918038318993406\n",
            "Pred:[0 0 1 1 0 1 1 0]\n",
            "True:[0 0 1 1 0 1 1 0]\n",
            "27 + 27 = 54\n",
            "------------\n",
            "iters:2200\n",
            "Loss:0.798385242419335\n",
            "Pred:[1 1 0 1 1 1 0 0]\n",
            "True:[1 1 0 1 0 1 0 0]\n",
            "110 + 102 = 220\n",
            "------------\n",
            "iters:2300\n",
            "Loss:0.8646947477743394\n",
            "Pred:[1 1 0 1 1 1 1 0]\n",
            "True:[1 0 0 1 0 1 1 0]\n",
            "34 + 116 = 222\n",
            "------------\n",
            "iters:2400\n",
            "Loss:0.4430860463157708\n",
            "Pred:[0 1 0 1 0 0 0 0]\n",
            "True:[0 1 0 1 0 0 0 0]\n",
            "36 + 44 = 80\n",
            "------------\n",
            "iters:2500\n",
            "Loss:0.7489456267073045\n",
            "Pred:[1 0 0 1 0 0 0 0]\n",
            "True:[1 0 0 1 0 1 1 0]\n",
            "74 + 76 = 144\n",
            "------------\n",
            "iters:2600\n",
            "Loss:0.6519088382080035\n",
            "Pred:[0 0 1 0 1 0 1 1]\n",
            "True:[0 0 1 1 1 0 1 1]\n",
            "39 + 20 = 43\n",
            "------------\n",
            "iters:2700\n",
            "Loss:0.3250327533119252\n",
            "Pred:[1 1 1 1 1 1 0 0]\n",
            "True:[1 1 1 1 1 1 0 0]\n",
            "125 + 127 = 252\n",
            "------------\n",
            "iters:2800\n",
            "Loss:0.44115139250609336\n",
            "Pred:[0 1 1 0 1 1 1 0]\n",
            "True:[0 1 1 0 1 0 1 0]\n",
            "87 + 19 = 110\n",
            "------------\n",
            "iters:2900\n",
            "Loss:0.4153624905624811\n",
            "Pred:[1 1 0 0 0 1 0 1]\n",
            "True:[1 1 0 0 0 1 0 1]\n",
            "77 + 120 = 197\n",
            "------------\n",
            "iters:3000\n",
            "Loss:0.6767877101313649\n",
            "Pred:[1 1 1 0 0 1 0 0]\n",
            "True:[1 0 0 0 0 1 0 0]\n",
            "18 + 114 = 228\n",
            "------------\n",
            "iters:3100\n",
            "Loss:0.6598350112570283\n",
            "Pred:[1 0 0 1 1 0 0 0]\n",
            "True:[0 1 1 1 1 0 0 0]\n",
            "20 + 100 = 152\n",
            "------------\n",
            "iters:3200\n",
            "Loss:0.5697709565426154\n",
            "Pred:[0 0 0 1 1 0 1 1]\n",
            "True:[0 0 0 1 1 1 1 1]\n",
            "26 + 5 = 27\n",
            "------------\n",
            "iters:3300\n",
            "Loss:0.391975356353481\n",
            "Pred:[1 1 0 1 1 1 0 0]\n",
            "True:[1 0 0 1 1 1 0 0]\n",
            "120 + 36 = 220\n",
            "------------\n",
            "iters:3400\n",
            "Loss:0.24698815074308453\n",
            "Pred:[0 1 1 0 1 1 1 0]\n",
            "True:[0 1 1 0 1 1 1 0]\n",
            "72 + 38 = 110\n",
            "------------\n",
            "iters:3500\n",
            "Loss:0.6371185058460529\n",
            "Pred:[1 1 1 0 1 0 0 0]\n",
            "True:[1 0 0 0 1 0 0 0]\n",
            "107 + 29 = 232\n",
            "------------\n",
            "iters:3600\n",
            "Loss:0.3607863854269744\n",
            "Pred:[1 1 1 0 0 0 1 0]\n",
            "True:[1 1 1 0 0 0 1 0]\n",
            "110 + 116 = 226\n",
            "------------\n",
            "iters:3700\n",
            "Loss:0.14042090738394847\n",
            "Pred:[1 0 1 1 0 0 1 0]\n",
            "True:[1 0 1 1 0 0 1 0]\n",
            "68 + 110 = 178\n",
            "------------\n",
            "iters:3800\n",
            "Loss:0.11668323969934075\n",
            "Pred:[1 0 1 1 0 0 1 1]\n",
            "True:[1 0 1 1 0 0 1 1]\n",
            "96 + 83 = 179\n",
            "------------\n",
            "iters:3900\n",
            "Loss:0.055185179856770517\n",
            "Pred:[0 1 0 0 1 1 0 1]\n",
            "True:[0 1 0 0 1 1 0 1]\n",
            "12 + 65 = 77\n",
            "------------\n",
            "iters:4000\n",
            "Loss:0.04627901480261627\n",
            "Pred:[0 0 1 0 1 1 1 1]\n",
            "True:[0 0 1 0 1 1 1 1]\n",
            "8 + 39 = 47\n",
            "------------\n",
            "iters:4100\n",
            "Loss:0.2863718854594314\n",
            "Pred:[1 0 1 0 1 0 1 1]\n",
            "True:[1 0 1 0 1 0 1 1]\n",
            "59 + 112 = 171\n",
            "------------\n",
            "iters:4200\n",
            "Loss:0.13606639836836795\n",
            "Pred:[0 1 1 1 0 1 1 1]\n",
            "True:[0 1 1 1 0 1 1 1]\n",
            "64 + 55 = 119\n",
            "------------\n",
            "iters:4300\n",
            "Loss:0.20092058077619748\n",
            "Pred:[0 1 1 0 0 1 1 1]\n",
            "True:[0 1 1 0 0 1 1 1]\n",
            "53 + 50 = 103\n",
            "------------\n",
            "iters:4400\n",
            "Loss:0.061891356173202516\n",
            "Pred:[1 1 1 0 1 1 1 0]\n",
            "True:[1 1 1 0 1 1 1 0]\n",
            "119 + 119 = 238\n",
            "------------\n",
            "iters:4500\n",
            "Loss:0.11706237726348448\n",
            "Pred:[0 1 0 1 0 1 1 1]\n",
            "True:[0 1 0 1 0 1 1 1]\n",
            "69 + 18 = 87\n",
            "------------\n",
            "iters:4600\n",
            "Loss:0.2791843239110346\n",
            "Pred:[1 0 0 1 0 0 0 0]\n",
            "True:[1 0 0 1 0 0 0 0]\n",
            "47 + 97 = 144\n",
            "------------\n",
            "iters:4700\n",
            "Loss:0.14702488639094582\n",
            "Pred:[0 1 1 0 1 0 0 1]\n",
            "True:[0 1 1 0 1 0 0 1]\n",
            "7 + 98 = 105\n",
            "------------\n",
            "iters:4800\n",
            "Loss:0.19340661694837266\n",
            "Pred:[1 0 0 0 0 1 1 0]\n",
            "True:[1 0 0 0 0 1 1 0]\n",
            "102 + 32 = 134\n",
            "------------\n",
            "iters:4900\n",
            "Loss:0.06094780943663904\n",
            "Pred:[1 1 0 1 1 0 1 0]\n",
            "True:[1 1 0 1 1 0 1 0]\n",
            "115 + 103 = 218\n",
            "------------\n",
            "iters:5000\n",
            "Loss:0.08207875906499548\n",
            "Pred:[1 0 0 1 1 1 0 0]\n",
            "True:[1 0 0 1 1 1 0 0]\n",
            "60 + 96 = 156\n",
            "------------\n",
            "iters:5100\n",
            "Loss:0.07713776273029795\n",
            "Pred:[1 1 0 1 0 1 1 1]\n",
            "True:[1 1 0 1 0 1 1 1]\n",
            "104 + 111 = 215\n",
            "------------\n",
            "iters:5200\n",
            "Loss:0.12567002299177096\n",
            "Pred:[0 1 1 1 1 1 1 1]\n",
            "True:[0 1 1 1 1 1 1 1]\n",
            "91 + 36 = 127\n",
            "------------\n",
            "iters:5300\n",
            "Loss:0.12776349032220194\n",
            "Pred:[1 0 1 0 1 1 0 1]\n",
            "True:[1 0 1 0 1 1 0 1]\n",
            "83 + 90 = 173\n",
            "------------\n",
            "iters:5400\n",
            "Loss:0.07636250384866851\n",
            "Pred:[0 1 1 0 1 0 0 0]\n",
            "True:[0 1 1 0 1 0 0 0]\n",
            "86 + 18 = 104\n",
            "------------\n",
            "iters:5500\n",
            "Loss:0.0317753514293261\n",
            "Pred:[0 0 1 1 1 1 0 0]\n",
            "True:[0 0 1 1 1 1 0 0]\n",
            "39 + 21 = 60\n",
            "------------\n",
            "iters:5600\n",
            "Loss:0.04830752019453449\n",
            "Pred:[0 0 1 0 0 0 1 1]\n",
            "True:[0 0 1 0 0 0 1 1]\n",
            "28 + 7 = 35\n",
            "------------\n",
            "iters:5700\n",
            "Loss:0.0659769563762127\n",
            "Pred:[0 1 0 0 0 1 1 1]\n",
            "True:[0 1 0 0 0 1 1 1]\n",
            "53 + 18 = 71\n",
            "------------\n",
            "iters:5800\n",
            "Loss:0.05160640673244129\n",
            "Pred:[1 0 1 0 0 1 0 1]\n",
            "True:[1 0 1 0 0 1 0 1]\n",
            "49 + 116 = 165\n",
            "------------\n",
            "iters:5900\n",
            "Loss:0.07470360837585632\n",
            "Pred:[1 0 0 0 0 1 0 1]\n",
            "True:[1 0 0 0 0 1 0 1]\n",
            "89 + 44 = 133\n",
            "------------\n",
            "iters:6000\n",
            "Loss:0.014884356533311792\n",
            "Pred:[1 0 1 0 0 1 1 1]\n",
            "True:[1 0 1 0 0 1 1 1]\n",
            "76 + 91 = 167\n",
            "------------\n",
            "iters:6100\n",
            "Loss:0.016954255477879394\n",
            "Pred:[1 0 0 1 1 1 0 1]\n",
            "True:[1 0 0 1 1 1 0 1]\n",
            "100 + 57 = 157\n",
            "------------\n",
            "iters:6200\n",
            "Loss:0.047028201595881045\n",
            "Pred:[1 0 0 1 0 0 1 1]\n",
            "True:[1 0 0 1 0 0 1 1]\n",
            "103 + 44 = 147\n",
            "------------\n",
            "iters:6300\n",
            "Loss:0.03180563593398571\n",
            "Pred:[0 1 0 0 1 0 1 0]\n",
            "True:[0 1 0 0 1 0 1 0]\n",
            "34 + 40 = 74\n",
            "------------\n",
            "iters:6400\n",
            "Loss:0.02477941328558209\n",
            "Pred:[0 1 0 0 1 1 1 0]\n",
            "True:[0 1 0 0 1 1 1 0]\n",
            "22 + 56 = 78\n",
            "------------\n",
            "iters:6500\n",
            "Loss:0.047777741554815296\n",
            "Pred:[1 0 0 0 0 1 0 1]\n",
            "True:[1 0 0 0 0 1 0 1]\n",
            "121 + 12 = 133\n",
            "------------\n",
            "iters:6600\n",
            "Loss:0.014808512043238558\n",
            "Pred:[1 0 1 0 0 1 1 0]\n",
            "True:[1 0 1 0 0 1 1 0]\n",
            "75 + 91 = 166\n",
            "------------\n",
            "iters:6700\n",
            "Loss:0.015869102251726767\n",
            "Pred:[1 1 0 1 1 0 0 1]\n",
            "True:[1 1 0 1 1 0 0 1]\n",
            "124 + 93 = 217\n",
            "------------\n",
            "iters:6800\n",
            "Loss:0.029498733547243496\n",
            "Pred:[1 0 0 0 0 1 0 0]\n",
            "True:[1 0 0 0 0 1 0 0]\n",
            "88 + 44 = 132\n",
            "------------\n",
            "iters:6900\n",
            "Loss:0.01870763384168331\n",
            "Pred:[1 1 0 1 0 1 0 0]\n",
            "True:[1 1 0 1 0 1 0 0]\n",
            "124 + 88 = 212\n",
            "------------\n",
            "iters:7000\n",
            "Loss:0.02621555829949044\n",
            "Pred:[0 0 1 0 0 1 1 1]\n",
            "True:[0 0 1 0 0 1 1 1]\n",
            "35 + 4 = 39\n",
            "------------\n",
            "iters:7100\n",
            "Loss:0.011111144800938466\n",
            "Pred:[0 1 1 1 0 1 0 0]\n",
            "True:[0 1 1 1 0 1 0 0]\n",
            "95 + 21 = 116\n",
            "------------\n",
            "iters:7200\n",
            "Loss:0.008847272362813673\n",
            "Pred:[1 1 0 0 0 1 0 1]\n",
            "True:[1 1 0 0 0 1 0 1]\n",
            "74 + 123 = 197\n",
            "------------\n",
            "iters:7300\n",
            "Loss:0.012161504032299547\n",
            "Pred:[1 0 1 1 0 1 0 1]\n",
            "True:[1 0 1 1 0 1 0 1]\n",
            "103 + 78 = 181\n",
            "------------\n",
            "iters:7400\n",
            "Loss:0.012763454812882019\n",
            "Pred:[0 1 0 1 1 0 1 1]\n",
            "True:[0 1 0 1 1 0 1 1]\n",
            "43 + 48 = 91\n",
            "------------\n",
            "iters:7500\n",
            "Loss:0.008124822002008902\n",
            "Pred:[0 1 0 1 0 1 0 1]\n",
            "True:[0 1 0 1 0 1 0 1]\n",
            "54 + 31 = 85\n",
            "------------\n",
            "iters:7600\n",
            "Loss:0.00914965410789576\n",
            "Pred:[0 1 0 0 0 1 0 1]\n",
            "True:[0 1 0 0 0 1 0 1]\n",
            "50 + 19 = 69\n",
            "------------\n",
            "iters:7700\n",
            "Loss:0.006019950305176156\n",
            "Pred:[0 1 0 1 1 1 0 1]\n",
            "True:[0 1 0 1 1 1 0 1]\n",
            "52 + 41 = 93\n",
            "------------\n",
            "iters:7800\n",
            "Loss:0.006687811753294008\n",
            "Pred:[1 0 1 1 0 1 0 1]\n",
            "True:[1 0 1 1 0 1 0 1]\n",
            "108 + 73 = 181\n",
            "------------\n",
            "iters:7900\n",
            "Loss:0.00802275000420163\n",
            "Pred:[0 1 0 1 0 1 1 1]\n",
            "True:[0 1 0 1 0 1 1 1]\n",
            "14 + 73 = 87\n",
            "------------\n",
            "iters:8000\n",
            "Loss:0.004483954799252642\n",
            "Pred:[0 0 1 1 1 1 1 0]\n",
            "True:[0 0 1 1 1 1 1 0]\n",
            "15 + 47 = 62\n",
            "------------\n",
            "iters:8100\n",
            "Loss:0.004944817458072105\n",
            "Pred:[1 1 1 0 1 1 0 0]\n",
            "True:[1 1 1 0 1 1 0 0]\n",
            "111 + 125 = 236\n",
            "------------\n",
            "iters:8200\n",
            "Loss:0.002585833822097517\n",
            "Pred:[1 0 0 1 1 0 1 0]\n",
            "True:[1 0 0 1 1 0 1 0]\n",
            "93 + 61 = 154\n",
            "------------\n",
            "iters:8300\n",
            "Loss:0.0012996330682396302\n",
            "Pred:[1 0 1 0 0 1 1 0]\n",
            "True:[1 0 1 0 0 1 1 0]\n",
            "49 + 117 = 166\n",
            "------------\n",
            "iters:8400\n",
            "Loss:0.005751585071267753\n",
            "Pred:[1 1 0 0 0 1 1 1]\n",
            "True:[1 1 0 0 0 1 1 1]\n",
            "88 + 111 = 199\n",
            "------------\n",
            "iters:8500\n",
            "Loss:0.007923988448912176\n",
            "Pred:[1 1 0 0 1 0 1 1]\n",
            "True:[1 1 0 0 1 0 1 1]\n",
            "93 + 110 = 203\n",
            "------------\n",
            "iters:8600\n",
            "Loss:0.012009818570715227\n",
            "Pred:[1 1 0 0 0 0 1 0]\n",
            "True:[1 1 0 0 0 0 1 0]\n",
            "120 + 74 = 194\n",
            "------------\n",
            "iters:8700\n",
            "Loss:0.010487997867228795\n",
            "Pred:[1 0 0 0 1 0 1 1]\n",
            "True:[1 0 0 0 1 0 1 1]\n",
            "122 + 17 = 139\n",
            "------------\n",
            "iters:8800\n",
            "Loss:0.00405374158092914\n",
            "Pred:[1 1 1 0 1 0 0 1]\n",
            "True:[1 1 1 0 1 0 0 1]\n",
            "108 + 125 = 233\n",
            "------------\n",
            "iters:8900\n",
            "Loss:0.005838869625617717\n",
            "Pred:[1 0 1 0 0 0 0 1]\n",
            "True:[1 0 1 0 0 0 0 1]\n",
            "90 + 71 = 161\n",
            "------------\n",
            "iters:9000\n",
            "Loss:0.007664382344949558\n",
            "Pred:[0 1 0 0 1 1 0 0]\n",
            "True:[0 1 0 0 1 1 0 0]\n",
            "26 + 50 = 76\n",
            "------------\n",
            "iters:9100\n",
            "Loss:0.006905436361175913\n",
            "Pred:[0 1 1 1 1 0 1 1]\n",
            "True:[0 1 1 1 1 0 1 1]\n",
            "91 + 32 = 123\n",
            "------------\n",
            "iters:9200\n",
            "Loss:0.005497108040035356\n",
            "Pred:[0 1 1 1 1 0 0 1]\n",
            "True:[0 1 1 1 1 0 0 1]\n",
            "43 + 78 = 121\n",
            "------------\n",
            "iters:9300\n",
            "Loss:0.005392676679204521\n",
            "Pred:[1 0 1 1 1 0 0 0]\n",
            "True:[1 0 1 1 1 0 0 0]\n",
            "108 + 76 = 184\n",
            "------------\n",
            "iters:9400\n",
            "Loss:0.0013227323532217083\n",
            "Pred:[1 1 0 0 1 0 0 0]\n",
            "True:[1 1 0 0 1 0 0 0]\n",
            "89 + 111 = 200\n",
            "------------\n",
            "iters:9500\n",
            "Loss:0.005355284860317819\n",
            "Pred:[1 0 0 0 1 1 0 1]\n",
            "True:[1 0 0 0 1 1 0 1]\n",
            "73 + 68 = 141\n",
            "------------\n",
            "iters:9600\n",
            "Loss:0.004804041398588376\n",
            "Pred:[1 1 0 0 1 1 1 1]\n",
            "True:[1 1 0 0 1 1 1 1]\n",
            "121 + 86 = 207\n",
            "------------\n",
            "iters:9700\n",
            "Loss:0.0045227585680813675\n",
            "Pred:[0 1 1 1 1 1 0 0]\n",
            "True:[0 1 1 1 1 1 0 0]\n",
            "56 + 68 = 124\n",
            "------------\n",
            "iters:9800\n",
            "Loss:0.004994389903406143\n",
            "Pred:[0 1 1 0 0 0 1 0]\n",
            "True:[0 1 1 0 0 0 1 0]\n",
            "54 + 44 = 98\n",
            "------------\n",
            "iters:9900\n",
            "Loss:0.005056830949601562\n",
            "Pred:[1 0 0 1 1 1 0 0]\n",
            "True:[1 0 0 1 1 1 0 0]\n",
            "110 + 46 = 156\n",
            "------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsBt7vxmO-bn"
      },
      "source": [
        "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "## [try] weight_init_stdやlearning_rate, hidden_layer_sizeを変更してみよう\n",
        "\n",
        "\n",
        "## [try] 重みの初期化方法を変更してみよう\n",
        "Xavier, He\n",
        "\n",
        "## [try] 中間層の活性化関数を変更してみよう\n",
        "ReLU(勾配爆発を確認しよう)<br>\n",
        "tanh(numpyにtanhが用意されている。導関数をd_tanhとして作成しよう)\n",
        "\n",
        "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
      ]
    }
  ]
}